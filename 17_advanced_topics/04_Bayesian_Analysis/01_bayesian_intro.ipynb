{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "# Introduction to Bayesian Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In general, statistical inference is the process of determining properties of a model/distribution given some data. \n",
    "\n",
    "- **Bayesian inference can be seen as the Bayesian counterpart to frequentist inference.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In frequentist inference, there is usually the notion of some true, unknown, parameter which is a constant and point estimates are inferred from data.\n",
    "\n",
    "\n",
    "To the contrary, Bayesian inference treats the model parameters as random variables and usually wants to deduce probabilistic statements about the distribution of parameters. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bayesian inference is the name given to several related interpretations of probability as an amount of epistemic confidence – the strength of beliefs, hypotheses etc. – rather than a frequency. This allows the application of probability to all sorts of propositions rather than just ones that come with a reference class. \n",
    "\n",
    "\"Bayesian\" has been used in this sense since about 1950. \n",
    "\n",
    "Since its rebirth in the 1950s, advancements in computing technology have allowed scientists from many disciplines to pair traditional Bayesian statistics with random walk techniques. The use of the Bayes theorem has been extended in science and in other fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><a>Bayes' Theorm</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bayesian inference utilizes the famous Bayes theorem:\n",
    "\n",
    "$$\n",
    " P(A|B) = \\frac{P(B | A) P(A)}{P(B)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thomas Bayes (c. 1701 – 7 April 1761) was an English statistician, philosopher and Presbyterian minister who is known for formulating a specific case of the theorem that bears his name: Bayes' theorem. \n",
    "\n",
    "Bayes never published what would become his most famous accomplishment; his notes were edited and published after his death by Richard Price, the 18th century nonconformist preacher and mathematician.\n",
    "\n",
    "Bayes was buried in Bunhill Fields burial ground in Moorgate, London, where many nonconformists lie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For model based inference, we can replace $A$ with the parameters $\\theta$ and $B$ with the data $D$ at interest. \n",
    "\n",
    "Furthermore, we can introduce $I$ which can be used to introduce an additional assumption (knowledge) to the inference such as which model to use.\n",
    "\n",
    "$$\n",
    " \\overbrace{P(\\theta| D, I)}^{\\text{posterior}} = \\frac{\\overbrace{P(D | \\theta, I)}^{\\text{likelihood}}\\overbrace{P(\\theta|I)}^{\\text{prior}}}{\\underbrace{P(D|I)}_{\\text{marginal likelihood}}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    " \\overbrace{P(\\theta| D, I)}^{\\text{posterior}} = \\frac{\\overbrace{P(D | \\theta, I)}^{\\text{likelihood}}\\overbrace{P(\\theta|I)}^{\\text{prior}}}{\\underbrace{P(D|I)}_{\\text{marginal likelihood}}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The prior distribution $P(\\theta|I)$ specifies our assumption about the parameters $\\theta$  before taking the data into account. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    " \\overbrace{P(\\theta| D, I)}^{\\text{posterior}} = \\frac{\\overbrace{P(D | \\theta, I)}^{\\text{likelihood}}\\overbrace{P(\\theta|I)}^{\\text{prior}}}{\\underbrace{P(D|I)}_{\\text{marginal likelihood}}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The likelihood $P(D | \\theta, I)$ represents the probability of the data if the parameters $\\theta$ are specified. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    " \\overbrace{P(\\theta| D, I)}^{\\text{posterior}} = \\frac{\\overbrace{P(D | \\theta, I)}^{\\text{likelihood}}\\overbrace{P(\\theta|I)}^{\\text{prior}}}{\\underbrace{P(D|I)}_{\\text{marginal likelihood}}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The marginal likelihood (or evidence) $P(D|I)$ is the distribution of the data $D$ given our additional assumption $I$. It is the normalization of the Bayes rule and plays an important rule for model comparison. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    " \\overbrace{P(\\theta| D, I)}^{\\text{posterior}} = \\frac{\\overbrace{P(D | \\theta, I)}^{\\text{likelihood}}\\overbrace{P(\\theta|I)}^{\\text{prior}}}{\\underbrace{P(D|I)}_{\\text{marginal likelihood}}}\n",
    "$$\n",
    "\n",
    "\n",
    "Finally, the posterior $P(\\theta| D, I)$ is the distribution of the parameters after taking the observed data $D$ and our additional (prior) assumption $I$ into account. We can also say that the posterior is proportional to the likelihood and the prior.\n",
    "\n",
    "$$\n",
    "posterior \\propto likelihood \\times prior\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><a>Bayesian vs Frequentist</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why should one use Bayesian inference, as opposed to classical inference? There are various answers. Broadly speaking, some of the arguments in favour of the the Bayesian approach are that it is:\n",
    "\n",
    "- fundamentally sound,\n",
    "- very flexible,\n",
    "- produces clear and direct inferences,\n",
    "- makes use of all available information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bayesian inference has a solid philosophical foundation. It is consistent with certain axioms of rational inference. \n",
    "\n",
    "Non-Bayesian systems of inference, such as fuzzy logic, must violate one or more of these axioms; their conclusions are rationally satisfying to the extent that they approximate Bayesian inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bayesian inference is at the same time rigid and flexible. \n",
    "\n",
    "It is rigid in the sense that all inference follows the same form: set up a likelihood and a prior, then calculate the posterior by conditioning on observed data via Bayes theorem. \n",
    "\n",
    "But this rigidity channels creativity into useful directions. It provides a template for setting up complex models when necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Frequentist inferences are awkward to explain. \n",
    "\n",
    "For example, confidence intervals and p-values are tedious to define rigorously. Most consumers of confidence intervals and p-values do not know what they mean and implicitly assume Bayesian interpretations. \n",
    "\n",
    "Particularly with regard to p-values, the common understanding can be grossly inaccurate. \n",
    "\n",
    "By contrast, Bayesian counterparts are simple to define and interpret. Bayesian credible intervals are exactly what most people think confidence intervals are. And a Bayesian hypotheses test simply compares the probability of each hypothesis via Bayes factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sometimes the necessity of specifying prior distributions is seen as a drawback to Bayesian inference. \n",
    "\n",
    "On the other hand, the ability to specify prior distributions means that more information can be incorporated in an inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a more thorough discussion about the differences between frequentist and Bayesian statistics, please refer to the following [blog post](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/) by Jake Vanderplas."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "css": [
   ""
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
