{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "24ca50a0-b9ad-425d-a28f-6bd3ca3ac378"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "-----\n",
    "\n",
    "# Introduction to Deep Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6287766f-972f-4b4d-bee7-5418f0af74de"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Deep learning** is one of the leading tools in data analysis these days and one of the most common frameworks for deep learning is **Keras**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3896ecb2-47fa-4dbc-a2ff-9c76e9dfe93e"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Deep learning allows computational models that are composed of multiple processing **layers** to learn representations of data with multiple levels of abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8c3060aa-fee9-438c-bc60-4685c0eb4750"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"assets/images/dl_overview.png\" >\n",
    "\n",
    "Credits: Yam Peleg ([@Yampeleg](https://twitter.com/yampeleg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5e13607b-3ec5-4a95-a2d8-f898f20748da"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h2><a>Building Blocks: Artificial Neural Networks (ANN)</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4fa2e86a-be32-4e78-96d9-f511a07e3908"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In machine learning and cognitive science, an artificial neural network (ANN) is a network inspired by biological neural networks which are used to estimate or approximate functions that can depend on a large number of inputs that are generally unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c25d7194-10bd-4196-9d4c-592bf6e188f9"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An ANN is based on a collection of connected units or nodes called artificial neurons which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. \n",
    "\n",
    "An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. \n",
    "\n",
    "The connections between artificial neurons are called 'edges'. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. \n",
    "\n",
    "Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. \n",
    "\n",
    "Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "15260f90-13d1-4fcc-afc6-379c507cb950"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An early version of ANN built from one node was called the **Perceptron**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "92d4603e-7e39-4156-818c-785df6189fe8"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"assets/images/Perceptron.png\" width=\"45%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "356d5ec7-3392-4daa-9671-4cc7111c5c91"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Perceptron is an algorithm for supervised learning of binary classifiers - functions that can decide whether an input (represented by a vector of numbers) belongs to one class or another.\n",
    "\n",
    "Much like logistic regression, the weights in a neural net are being multiplied by the input vector summed up and feeded into the activation function's input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><a>Single Layer Neural Network</a></h2>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"assets/images/single_layer.png\" width=\"65%\" />\n",
    "\n",
    "_(Source: Python Machine Learning, S. Raschka)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Weights Update Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We use a **gradient descent** optimization algorithm to learn the _Weights Coefficients_ of the model.\n",
    "<br><br>\n",
    "- In every **epoch** (pass over the training set), we update the weight vector $w$ using the following update rule:\n",
    "\n",
    "$$\n",
    "w = w + \\Delta w, \\text{where } \\Delta w = - \\eta \\nabla J(w)\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "In other words, we computed the gradient based on the whole training set and updated the weights of the model by taking a step into the **opposite direction** of the gradient $ \\nabla J(w)$. \n",
    "\n",
    "In order to fin the **optimal weights of the model**, we optimized an objective function (e.g. the Sum of Squared Errors (SSE)) cost function $J(w)$. \n",
    "\n",
    "Furthermore, we multiply the gradient by a factor, the learning rate $\\eta$ , which we choose carefully to balance the **speed of learning** against the risk of overshooting the global minimum of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In **gradient descent optimization**, we update all the **weights simultaneously** after each epoch, and we define the _partial derivative_ for each weight $w_j$ in the weight vector $w$ as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_j} J(w) = \\sum_{i} ( y^{(i)} - a^{(i)} )  x^{(i)}_j\n",
    "$$\n",
    "\n",
    "**Note**: _The superscript $(i)$ refers to the i-th sample. The subscript $j$ refers to the j-th dimension/feature_\n",
    "\n",
    "\n",
    "Here $y^{(i)}$ is the target class label of a particular sample $x^{(i)}$ , and $a^{(i)}$ is the **activation** of the neuron \n",
    "\n",
    "(which is a linear function in the special case of _Perceptron_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We define the **activation function** $\\phi(\\cdot)$ as follows:\n",
    "\n",
    "$$\n",
    "\\phi(z) = z = a = \\sum_{j} w_j x_j = \\mathbf{w}^T \\mathbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h2><a>Introducing the multi-layer neural network architecture</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we will see how to connect **multiple single neurons** to a **multi-layer feedforward neural network**; this special type of network is also called a **multi-layer perceptron** (MLP). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"assets/images/multi-layers-1.png\" width=\"50%\" />\n",
    "\n",
    "The figure shows the concept of an **MLP** consisting of three layers: one _input_ layer, one _hidden_ layer, and one _output_ layer. \n",
    "\n",
    "The units in the hidden layer are fully connected to the input layer, and the output layer is fully connected to the hidden layer, respectively. \n",
    "\n",
    "If such a network has **more than one hidden layer**, we also call it a **deep artificial neural network**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MLPs are examples of a fully connected NN.\n",
    "\n",
    "In a fully connected layer each neuron is connected to every neuron in the previous layer, and each connection has it's own weight. \n",
    "\n",
    "This is a totally general purpose connection pattern and makes no assumptions about the features in the data. \n",
    "\n",
    "It's also very expensive in terms of memory (weights) and computation (connections)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"assets/images/multi-layers-2.png\" width=\"50%\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"assets/images/neural_net_flowchart.png\" width=\"40%\">\n",
    "\n",
    "* Starting at the input layer, we forward propagate the patterns of the training data through the network to generate an output.\n",
    "\n",
    "\n",
    "\n",
    "* Based on the network's output, we calculate the error that we want to minimize using a cost function that we will describe later.\n",
    "\n",
    "\n",
    "\n",
    "* We backpropagate the error, find its derivative with respect to each weight in the network, and update the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sigmoid Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"assets/images/logistic_function.png\" width=\"50%\" />\n",
    "\n",
    "Neural networks are somewhat related to logistic regression. Basically, we can think of logistic regression as a one layer neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"assets/images/schematic_log_reg.png\" width=\"50%\" />\n",
    "\n",
    "In fact, it is very common to use logistic sigmoid functions as activation functions in the hidden layer of a neural network – like the schematic above but without the threshold function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It’s fine to use the threshold function in the output layer if we have a binary classification task (in this case, you’d only have one sigmoid unit in the output layer). \n",
    "\n",
    "In the case of multi-class classification, we can use a generalization of the One-vs-All approach; i.e., we encode your target class labels via one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, we would encode the three class labels in the familiar Iris dataset (0=Setosa, 1=Versicolor, 2=Virginica).\n",
    "\n",
    "Then, for the prediction step after learning the model, we just return the “argmax,” the index in the output vector with the highest value as the class label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That’s fine if we are only interested in the class label prediction. \n",
    "\n",
    "Now, if we want “meaningful” class probabilities, that is, class probabilities that sum up to 1, we could use the softmax function (aka “multinomial logistic regression”). \n",
    "\n",
    "In softmax, the probability of a particular sample with net input z belongs to the i th class can be computed with a normalization term in the denominator that is the sum of all M linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "5678486b-caf4-440b-be62-2f1286982c71"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The weights of each neuron are learned by **gradient descent**, where each neuron's error is derived with respect to it's weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Optimization is done for each layer with respect to the previous layer in a technique known as **BackPropagation**.\n",
    "\n",
    "<img src=\"assets/images/backprop.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"assets/images/neural_net_flowchart.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One of the nice properties of logistic regression is that the logistic cost function (or max-entropy) is convex, and thus we are guaranteed to find the global cost minimum. \n",
    "\n",
    "But, once we stack logistic activation functions in a multi-layer neural network, we’ll lose this convexity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looking only at a single weight / model coefficient, we can picture the cost function in a multi-layer perceptron as a rugged landscape with multiple local minima that can trap the optimization algorithm:\n",
    "\n",
    "<img src=\"assets/images/unconvex.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, in practice, backpropagation works quite well for 1 or 2 layer neural networks (and there are deep learning algos such as autoencoders) to help with deeper architectures. \n",
    "\n",
    "Even if you may likely converge to a local minima, you often still end up with a powerful predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h2><a>Recap</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"assets/images/neural_net_flowchart.png\" width=\"50%\">\n",
    "\n",
    "Training a neural network revolves around the following objects:\n",
    "- Layers, which are combined into a network (or model)\n",
    "- The input data and corresponding targets\n",
    "- The loss function, which defines the feedback signal used for learning\n",
    "- The optimizer, which determines how learning proceeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"assets/images/neural_net_flowchart.png\" width=\"50%\">\n",
    "\n",
    "The network, composed of layers that are chained together, maps the input data to predictions. \n",
    "\n",
    "The loss function then compares these predictions to the targets, producing a loss value: a measure\n",
    "of how well the network’s predictions match what was expected. \n",
    "\n",
    "The optimizer uses this loss value to update the network’s weights."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbpresent": {
   "slides": {
    "5445cab1-b2b9-4492-a3be-c4063bd67610": {
     "id": "5445cab1-b2b9-4492-a3be-c4063bd67610",
     "prev": "ef2af7e1-6294-42cf-a434-b1e17cf679a7",
     "regions": {
      "4baa3e75-a346-47d1-a015-c9040545adbd": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "8c3060aa-fee9-438c-bc60-4685c0eb4750",
        "part": "whole"
       },
       "id": "4baa3e75-a346-47d1-a015-c9040545adbd"
      },
      "bde8ff6d-7eb8-4d83-b6ba-f57b94dc889d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3896ecb2-47fa-4dbc-a2ff-9c76e9dfe93e",
        "part": "whole"
       },
       "id": "bde8ff6d-7eb8-4d83-b6ba-f57b94dc889d"
      }
     }
    },
    "d263b7c7-85dc-422a-aec7-fa2c09bc1d23": {
     "id": "d263b7c7-85dc-422a-aec7-fa2c09bc1d23",
     "prev": "ff705989-f5c1-46a1-8f7a-0fe14a949357",
     "regions": {
      "053c599b-2e88-4270-bc7b-1299a2a2bd20": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4fa2e86a-be32-4e78-96d9-f511a07e3908",
        "part": "whole"
       },
       "id": "053c599b-2e88-4270-bc7b-1299a2a2bd20"
      }
     }
    },
    "e3153a08-42da-4785-a596-7a9ebb6a6f19": {
     "id": "e3153a08-42da-4785-a596-7a9ebb6a6f19",
     "prev": "5445cab1-b2b9-4492-a3be-c4063bd67610",
     "regions": {
      "215ebdb3-236e-4a0d-aa65-6b4fb839e5b3": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "2f1c6299-954a-461a-b5c1-a86a1d12ad15",
        "part": "whole"
       },
       "id": "215ebdb3-236e-4a0d-aa65-6b4fb839e5b3"
      },
      "6b33306d-283d-47ef-afec-1f8b5e7fa9a5": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6287766f-972f-4b4d-bee7-5418f0af74de",
        "part": "whole"
       },
       "id": "6b33306d-283d-47ef-afec-1f8b5e7fa9a5"
      }
     }
    },
    "edcbed78-ec6f-4c1d-84f0-fb1601a07f1d": {
     "id": "edcbed78-ec6f-4c1d-84f0-fb1601a07f1d",
     "prev": "f912a9b4-1188-440a-b13e-86224d1dada5",
     "regions": {
      "b619970a-3b72-4add-88af-158f75e81f2f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "356d5ec7-3392-4daa-9671-4cc7111c5c91",
        "part": "whole"
       },
       "id": "b619970a-3b72-4add-88af-158f75e81f2f"
      },
      "d86b2236-e08b-44cf-9298-0c818a1c9c88": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "5678486b-caf4-440b-be62-2f1286982c71",
        "part": "whole"
       },
       "id": "d86b2236-e08b-44cf-9298-0c818a1c9c88"
      }
     }
    },
    "ef2af7e1-6294-42cf-a434-b1e17cf679a7": {
     "id": "ef2af7e1-6294-42cf-a434-b1e17cf679a7",
     "prev": "f59d310a-ea9f-4360-8151-6ea1c66a66ac",
     "regions": {
      "e33b4032-a118-4ab7-920b-5cd4c1bb0523": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "24ca50a0-b9ad-425d-a28f-6bd3ca3ac378",
        "part": "whole"
       },
       "id": "e33b4032-a118-4ab7-920b-5cd4c1bb0523"
      }
     }
    },
    "f59d310a-ea9f-4360-8151-6ea1c66a66ac": {
     "id": "f59d310a-ea9f-4360-8151-6ea1c66a66ac",
     "prev": null,
     "regions": {}
    },
    "f607e60a-8fc2-4061-bbbe-fb75f498cec7": {
     "id": "f607e60a-8fc2-4061-bbbe-fb75f498cec7",
     "prev": "d263b7c7-85dc-422a-aec7-fa2c09bc1d23",
     "regions": {
      "cea17e10-48c1-4b1a-a39e-5462b17adc89": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "df0121bc-10f1-4ace-840e-6fc89c6fdc7f",
        "part": "whole"
       },
       "id": "cea17e10-48c1-4b1a-a39e-5462b17adc89"
      },
      "fe327edc-53cc-4a66-b242-ffbc1148099f": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "c25d7194-10bd-4196-9d4c-592bf6e188f9",
        "part": "whole"
       },
       "id": "fe327edc-53cc-4a66-b242-ffbc1148099f"
      }
     }
    },
    "f912a9b4-1188-440a-b13e-86224d1dada5": {
     "id": "f912a9b4-1188-440a-b13e-86224d1dada5",
     "prev": "f607e60a-8fc2-4061-bbbe-fb75f498cec7",
     "regions": {
      "213f5824-f494-4cf6-8458-5dd5d629794c": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "92d4603e-7e39-4156-818c-785df6189fe8",
        "part": "whole"
       },
       "id": "213f5824-f494-4cf6-8458-5dd5d629794c"
      },
      "b141fdfe-4270-4293-a445-2b5642daf56e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "15260f90-13d1-4fcc-afc6-379c507cb950",
        "part": "whole"
       },
       "id": "b141fdfe-4270-4293-a445-2b5642daf56e"
      }
     }
    },
    "ff705989-f5c1-46a1-8f7a-0fe14a949357": {
     "id": "ff705989-f5c1-46a1-8f7a-0fe14a949357",
     "prev": "e3153a08-42da-4785-a596-7a9ebb6a6f19",
     "regions": {
      "fe64f274-127a-4ab1-9a01-d011d67aa8fb": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5e13607b-3ec5-4a95-a2d8-f898f20748da",
        "part": "whole"
       },
       "id": "fe64f274-127a-4ab1-9a01-d011d67aa8fb"
      }
     }
    }
   },
   "themes": {
    "default": "cdfd905a-2df0-447c-8f49-becf28e8e1b1",
    "theme": {
     "cdfd905a-2df0-447c-8f49-becf28e8e1b1": {
      "id": "cdfd905a-2df0-447c-8f49-becf28e8e1b1",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         155,
         177,
         192
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410"
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 8
       },
       "h2": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "font-family": "Merriweather",
       "font-size": 4
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
