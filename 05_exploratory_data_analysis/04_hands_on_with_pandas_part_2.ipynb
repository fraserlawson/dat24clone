{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Hands on with `pandas`: Part 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Objective:** Know how to handle null and missing values.\n",
    "\n",
    "Sometimes, values will be missing from the source data or as a byproduct of manipulations. It is very important to detect missing data. Missing data can:\n",
    "\n",
    "- Make the entire row ineligible to be training data for a model.\n",
    "- Hint at data-collection errors.\n",
    "- Indicate improper conversion or manipulation.\n",
    "- Actually not be missing — it sometimes means \"zero,\" \"false,\" \"not applicable,\" or \"entered an empty string.\"\n",
    "\n",
    "For example, a `.csv` file might have a missing value in some data fields:\n",
    "\n",
    "```\n",
    "tool_name,material,cost\n",
    "hammer,wood,8\n",
    "chainsaw,,\n",
    "wrench,metal,5\n",
    "```\n",
    "\n",
    "When this data is imported, \"null\" values will be stored in the second row (in the \"material\" and \"cost\" columns).\n",
    "\n",
    "> In Pandas, a \"null\" value is either `None` or `np.NaN` (Not a Number). Many fixed-size numeric datatypes (such as integers) do not have a way of representing `np.NaN`. So, numeric columns will be promoted to floating-point datatypes that do support it. For example, when importing the `.csv` file above:\n",
    "\n",
    "> - **For the second row:** `None` will be stored in the \"material\" column and `np.NaN` will be stored in the \"cost\" column. The entire \"cost\" column (stored as a single `ndarray`) must be stored as floating-point values to accommodate the `np.NaN`, even though an integer `8` is in the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "drinks = pd.read_csv(\"data/drinks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values are usually excluded in calculations by default.\n",
    "drinks[\"continent\"].value_counts()              \n",
    "# Excludes missing values in the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes missing values\n",
    "drinks[\"continent\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find missing values in a Series.\n",
    "# True if missing, False if not missing\n",
    "drinks[\"continent\"].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the missing values — sum() works because True is 1 and False is 0.\n",
    "drinks[\"continent\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only show rows where continent is not missing.\n",
    "drinks[drinks[\"continent\"].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Pandas Axis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sums \"down\" the 0 axis (rows) — so, we get the sums of each column\n",
    "drinks.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis=0 is the default.\n",
    "drinks.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sums \"across\" the 1 axis (columns) — \n",
    "# so, we get the sums of numeric values in the row \n",
    "# (beer+spirit+wine+liters+…)\n",
    "drinks.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find missing values in a `DataFrame`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the missing values in each column — remember by default, axis=0.\n",
    "print((drinks.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row if ANY values are missing from any column — can be dangerous!\n",
    "drinks.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row only if ALL values are missing.\n",
    "drinks.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filling Missing Values**<br>\n",
    "You may have noticed that the continent North America (NA) does not appear in the `continent` column. Pandas read in the original data and saw \"NA\", thought it was a missing value, and converted it to a `NaN`, missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill in missing values with \"NA\" — \n",
    "# this is dangerous to do without manually verifying them!\n",
    "drinks[\"continent\"].fillna(value='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifies \"drinks\" in-place\n",
    "drinks[\"continent\"].fillna(value='NA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off the missing value filter — this is a better approach!\n",
    "drink_cols = ['country', 'beer', 'spirit', 'wine', 'litres', 'continent']\n",
    "drinks = pd.read_csv('data/drinks.csv', header=0, \n",
    "                     names=drink_cols, na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinks[\"continent\"].value_counts()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"split-apply-combine\"></a>\n",
    "### Split-Apply-Combine\n",
    "\n",
    "Split-apply-combine is a pattern for analyzing data. Suppose we want to find mean beer consumption per country. Then:\n",
    "\n",
    "- **Split:** We group data by continent.\n",
    "- **Apply:** For each group, we apply the `mean()` function to find the average beer consumption.\n",
    "- **Combine:** We now combine the continent names with the `mean()`s to produce a summary of our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each continent, calculate the mean beer servings.\n",
    "drinks.groupby('continent')[\"beer\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each continent, calculate the mean of all numeric columns.\n",
    "drinks.groupby('continent').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each continent, describe beer servings.\n",
    "drinks.groupby('continent')[\"beer\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar, but outputs a DataFrame and can be customized \n",
    "# — \"agg\" allows you to aggregate results of Series functions\n",
    "drinks.groupby('continent')[\"beer\"].agg(['count', 'mean', 'min', 'max'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinks.groupby('continent')[\"beer\"].agg(['count', 'mean', \n",
    "                                         'min', 'max']).sort_values('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each continent, describe all numeric columns.\n",
    "drinks.groupby('continent').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n",
    "                       'foo', 'bar', 'foo', 'foo'],\n",
    "                       'B' : ['one', 'one', 'two', 'three',\n",
    "                             'two', 'two', 'one', 'three'],\n",
    "                       'C' : np.random.randn(8),\n",
    "                       'D' : np.random.randn(8)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a pivot table: we'll use the contents of column A as the new row index, column B as the new column index, and use C and D to populate these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot Tables\n",
    "pd.pivot_table(df,values=['C','D'],index=['A'],columns=['B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"joining-dataframes\"></a>\n",
    "### Joining (Merging) `DataFrames`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use pandas to merge multiple data frames. First a list of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_cols = ['movie_id', 'title']\n",
    "movies_filename = 'data/movies.tbl'\n",
    "\n",
    "movies = pd.read_table(movies_filename,\n",
    "                       sep='|',\n",
    "                       header=None,\n",
    "                       names=movie_cols,\n",
    "                       usecols=[0, 1],\n",
    "                       encoding=\"latin-1\")\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, import their user ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings_filename = 'data/movie_ratings.tsv'\n",
    "\n",
    "ratings = pd.read_table(ratings_filename, sep='\\t', \n",
    "                        header=None, names=rating_cols)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge \"movies\" and \"ratings\" (inner join on \"movie_id\")\n",
    "\n",
    "- if you leave `on` blank, `pandas` will try and work out common columns - but it's better to be explicit!\n",
    "- `how` defaults to \"inner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings = pd.merge(movies, ratings, on=\"movie_id\", how=\"inner\")\n",
    "movie_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movie_ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"multiple-columns\"></a>\n",
    "### Selecting Multiple Columns and Filtering Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple columns — \n",
    "# yet another overload of the DataFrame indexing operator!\n",
    "ufo = pd.read_csv(\"data/ufo.csv\")\n",
    "my_cols = ['City', 'State']     # Create a list of column names...\n",
    "ufo[my_cols]                    # ...and use that list to select columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, combine into a single step \n",
    "# (this is a Python list inside of the Python index operator!).\n",
    "ufo[['City', 'State']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use `loc` to select columns by name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"loc\" locates the values from the \n",
    "# first parameter (colon means \"all rows\"), and the column \"City\".\n",
    "ufo.loc[:, 'City']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two columns.\n",
    "ufo.loc[:, ['City', 'State']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a range of columns — \n",
    "# unlike Python ranges, Pandas index ranges INCLUDE the \n",
    "# final column in the range.\n",
    "ufo.loc[:, 'City':'State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"loc\" can also filter rows by \"name\" (the index).\n",
    "# Row 0, all columns\n",
    "ufo.loc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows 0/1/2, all columns\n",
    "ufo.loc[0:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows 0/1/2, range of columns\n",
    "ufo.loc[0:2, 'City':'State'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"iloc\" to filter rows and select columns by integer position.\n",
    "# (Remember that rows/columns use indices, so \"iloc\" lets you refer \n",
    "# to indices via their index rather than value!)\n",
    "# All rows, columns in position 0/3 (City/State)\n",
    "ufo.iloc[:, [0, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All rows, columns in position 0/1/2/3\n",
    "# Note here it is NOT INCLUDING 4 because this is an integer range, \n",
    "# not a Pandas index range!\n",
    "ufo.iloc[:, 0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows in position 0/1/2, all columns\n",
    "ufo.iloc[0:3, :] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"other-features\"></a>\n",
    "### Other Commonly Used Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply an arbitrary function to each value of a Pandas column, \n",
    "# storing the result in a new column.\n",
    "users = pd.read_table('data/user.tbl', sep='|')\n",
    "users['under30'] = users[\"age\"].apply(lambda age: age < 30)\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply an arbitrary function to each row of a DataFrame, storing \n",
    "# the result in a new column.\n",
    "#  (Remember that, by default, axis=0. Since we want to go row by \n",
    "# row, we set axis=1.)\n",
    "users['under30male'] = users.apply(lambda row: row[\"age\"] < 30 \n",
    "                                   and row[\"gender\"] == 'M', axis=1)\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map existing values to a different set of values.\n",
    "users['is_male'] = users[\"gender\"].map({'F':0, 'M':1})\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all instances of a value in a column \n",
    "# (must match entire value).\n",
    "ufo[\"State\"].replace('Fl', 'FL', inplace=True)\n",
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# String methods are accessed via \"str\".\n",
    "ufo[\"State\"].str.upper()    # Converts to upper case\n",
    "# checks for a substring\n",
    "ufo['Colors Reported'].str.contains('RED', na='False') \n",
    "ufo.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a string to the datetime format \n",
    "# (this is often slow — consider doing it in the \"read_csv()\" method.)\n",
    "ufo[\"Time\"] = pd.to_datetime(ufo[\"Time\"])\n",
    "ufo[\"Time\"].dt.hour # Datetime format exposes convenient attributes\n",
    "\n",
    "(ufo[\"Time\"].max() - ufo[\"Time\"].min()).days  \n",
    "# Also allows you to do datetime \"math\"\n",
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the datatype of a column.\n",
    "drinks['beer'] = drinks[\"beer\"].astype('float')\n",
    "drinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for \"continent\" and exclude first dummy column.\n",
    "continent_dummies = pd.get_dummies(drinks[\"continent\"], \n",
    "                                   prefix='cont').iloc[:, 1:]\n",
    "continent_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate two DataFrames (axis=0 for rows, axis=1 for columns).\n",
    "pd.concat([drinks, continent_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"uncommon-features\"></a>\n",
    "### Other Handy Features of Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Detecting duplicate rows\n",
    "users.duplicated()  # True if a row is identical to a previous row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.duplicated().sum()    # Count of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[users.duplicated()]   # Only show duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.drop_duplicates()     # Drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users[\"age\"].duplicated()      # Check a single column for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.duplicated(['age', 'gender', 'zip_code']).sum()   \n",
    "# Specify columns for finding duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a range of values into descriptive groups.\n",
    "drinks['beer_level'] = 'low'    # Initially set all values to \"low\"\n",
    "\n",
    "# Change 101-200 to \"med\"\n",
    "drinks.loc[drinks[\"beer\"].between(101, 200), 'beer_level'] = 'med' \n",
    "# Change 201-400 to \"high\"\n",
    "drinks.loc[drinks[\"beer\"].between(201, 400), 'beer_level'] = 'high'    \n",
    "drinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a cross-tabulation of two Series.\n",
    "pd.crosstab(drinks[\"continent\"], drinks[\"beer_level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"beer_level\" into the \"category\" datatype.\n",
    "drinks['beer_level'] = pd.Categorical(drinks[\"beer_level\"], \n",
    "                                      categories=['low', 'med', 'high'])\n",
    "\n",
    "# Sorts by the categorical ordering (low to high)\n",
    "drinks.sort_values('beer_level')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit which rows are read when reading in a file — \n",
    "# useful for large files!\n",
    "\n",
    "# Only read first 10 rows\n",
    "pd.read_csv('data/drinks.csv', nrows=10)           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the first two rows of data\n",
    "pd.read_csv('data/drinks.csv', skiprows=[1, 2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a DataFrame out to a .csv\n",
    "drinks.to_csv('drinks_updated.csv') # Index is used as first column\n",
    "drinks.to_csv('drinks_updated.csv', index=False)    # Ignore index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from a dictionary.\n",
    "pd.DataFrame({'capital':['Montgomery', 'Juneau', 'Phoenix'], \n",
    "              'state':['AL', 'AK', 'AZ']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from a list of lists.\n",
    "pd.DataFrame([['Montgomery', 'AL'], ['Juneau', 'AK'], ['Phoenix', 'AZ']], \n",
    "             columns=['capital', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample a DataFrame.\n",
    "\n",
    "# Create a Series of Booleans\n",
    "mask = np.random.rand(len(drinks)) < 0.66   \n",
    "\n",
    "# Will contain around 66% of the rows\n",
    "train = drinks[mask]        \n",
    "\n",
    "# Will contain the remaining rows\n",
    "test = drinks[~mask]                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the maximum number of rows and columns printed \n",
    "# ('None' means unlimited).\n",
    "pd.set_option('max_rows', None)     # Default is 60 rows\n",
    "pd.set_option('max_columns', None)  # Default is 20 columns\n",
    "print(drinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset options to defaults.\n",
    "pd.reset_option('max_rows')\n",
    "pd.reset_option('max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the options temporarily \n",
    "# (settings are restored when you exit the \"with\" block).\n",
    "with pd.option_context('max_rows', None, 'max_columns', None):\n",
    "    print(drinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "### Summary\n",
    "\n",
    "Believe it or not, we've only barely touched the surface of everything that Pandas offers. Don't worry if you don't remember most of it — for now, just knowing what exists is key. Remember that the more you use Pandas to manipulate data, the more of these functions you will take interest in, look up, and remember.\n",
    "\n",
    "In these notebooks, the most important things to familiarise yourself with are the basics:\n",
    "- Manipulating `DataFrames` and `Series`\n",
    "- Filtering columns and rows\n",
    "- Handling missing values\n",
    "- Split-apply-combine (this one takes some practice!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
