{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Linear Regression: Further Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will build on the bikes dataset we used last time, this time with more \"best practice\".\n",
    "\n",
    "Read in the bikes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Choose 3 features to predict `total_rentals` and put them in variables X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Create a training and test set\n",
    "\n",
    "We'll be using the training set for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: Measure the performance of your model across 7 folds\n",
    "\n",
    "Get a feel for your model's performance. Try both RMSE (using `'neg_mean_squared_error'`) and Mean Absolute Error (MAE) using `'neg_mean_absolute_error'` as scoring metrics and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4: We'll try two more models\n",
    "\n",
    "First, do a new train-test split on the entire bikes data.\n",
    "\n",
    "Previously, we only used 3 features for our split, but we want access to all of them now, so we can train different models that use different combinations of features.\n",
    "\n",
    "We will do both our cross-validated training and model evaluation on our **training** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try **two** more models with different combinations of features and compare their performance.\n",
    "\n",
    "*As bonus practice, you could put the code you've written so far into a function so you can easily try different combinations of features!*\n",
    "\n",
    "*For example, it could take as parameters a feature matrix X (your training set) and y (your training targets) and a list of columns to use for the model, and could print/return the cross-validated scores.*\n",
    "\n",
    "```python\n",
    "def evaluate_features(features, X, y):\n",
    "    ...```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5: Take the best of your three trained models and evaluate it on the *test* set.\n",
    "\n",
    "This is to get an estimate of how well your model performs in the real world. It would be your final reported accuracy score. After this step, you should **not** train-test on the same data anymore, because you will be prone to overfitting.\n",
    "\n",
    "For this question, first use your best model to predict values on the test inputs (X_test) and compare to the actual values (y_test).\n",
    "\n",
    "(*Note: if you have a model object from step 4, but used `cross_val_score` to evaluate performance, you will need to fit the model again because `cross_val_score` doesn't do this for you.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6: How did your model do?\n",
    "\n",
    "If your test set error is similar to the training/cross-validated error, it means your training accuracy was representative of the model's real world performance.\n",
    "\n",
    "Overfitting happens when your test error is much higher than your training error - i.e. your model hasn't generalised.\n",
    "\n",
    "Look at the output from **5** - how well did your model do \"in the real world\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
