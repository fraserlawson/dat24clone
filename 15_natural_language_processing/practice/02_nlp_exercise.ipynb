{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# NLP Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's exercise we have ~15,000 tweets aimed at various US airlines. Their sentiments are pre-labelled, and it's your task to build a classifier that can predict a tweet's sentiment based on its text.\n",
    "\n",
    "#### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../assets/data/airline_tweets.csv\", encoding=\"latin-1\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Exploration\n",
    "\n",
    "- how many airlines are there?\n",
    "- what is the proportion of sentiment across tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Data cleaning\n",
    "\n",
    "- tidy up the tweets as you see fit\n",
    "- you might want to remove Twitter handles for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def clean_up_tweet(tweet):\n",
    "    # step 1: remove handles and stitch back into a single string\n",
    "    words_no_handles = [w.lower() for w in tweet.split() if not w.startswith(\"@\")]\n",
    "    # step 2: stemming\n",
    "    return \" \".join([stemmer.stem(w) for w in words_no_handles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: Train-test split\n",
    "\n",
    "Do a train-test split so we can test our best algorithm at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4: Try a simple binary count-based model\n",
    "\n",
    "- Transform your raw text into binary features\n",
    "- *Hint: it's a simple parameter change in `CountVectorizer`*\n",
    "- Choose an appropriate machine learning model for the task\n",
    "- Use cross-validation to evaluate your model's performance on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5: Try playing around with some of the options in `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6: Try moving to a TF-IDF model. Can you improve on your best score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7: Evaluate your best model (the one with the highest cross-validated score) on your test set. What is its final performance?\n",
    "\n",
    "When you evaluate your model on the test set, remember to **only fit your Vectorizer on the training set** and use the fit Vectorizer to transform the test set. This is just like the rule for using z-score standardisation. We don't want the test set to interfere in our data transformations.\n",
    "\n",
    "Also make sure to look at more than just a single score - consider printing the confusion matrix for example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conmat = np.array(confusion_matrix(y_test, rf.predict(X_test_text)))\n",
    "confusion = pd.DataFrame(conmat, index=['negative', 'neutral', 'positive'],\\\n",
    "                     columns=['Pred neg', 'Pred neutral', 'Pred pos'])\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "heat = sns.heatmap(confusion, annot=True, annot_kws={\"size\": 20},cmap='Blues',fmt='g', cbar=False)\n",
    "plt.xticks(rotation=0, fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title(\"Confusion Matrix\", fontsize=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
