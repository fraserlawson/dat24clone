{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Latent variable models\n",
    " \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many advances in NLP are based on using data to learn rules of grammar and language.  We saw these tools earlier.\n",
    "\n",
    "- Tokenization\n",
    "\n",
    "- Stemming or lemmatization\n",
    "\n",
    "- Parsing and tagging\n",
    "\n",
    "Each of these are based on a classical or theoretical understanding of language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Latent variable models are models in which we assume the data we are observing has some hidden, underlying structure that we can’t see, and which we’d like to learn.\n",
    "\n",
    "These hidden, underlying structures are the latent (i.e. hidden) variables we want our model to understand.\n",
    "\n",
    "Text processing is a common application of latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Latent variable models are different in that they try to understand language based on how the words are used.\n",
    "\n",
    "- For example, instead of learning that ‘bad’ and ‘badly’ are related because they share the same root, we’ll determine that they are related because they are often used in the same way often or near the same words.\n",
    "\n",
    "We’ll use unsupervised techniques (discovering patterns or structure) to extract the information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Traditional NLP Models |      Latent Variable Models   |\n",
    "|------|------|\n",
    "|  ‘bad’ and ‘badly’ are related because they share a common root. | ‘bad’ and ‘badly’ are related because they are used the same way or near the same words.|\n",
    "| | |\n",
    "| ‘Python’ and ‘C++’ are both programming languages because \u000b",
    "they are often a noun preceded by \u000b",
    "the verb ‘program’ or ‘code’. | ‘Python’ and ‘C++’ are both programming languages because they are often used in the same context.| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Latent variable techniques are often used for recommending news articles or mining large troves of data to find commonalities.\n",
    "\n",
    "Topic modeling, a method we’ll cover today, is used in the NY times recommendation engine.\n",
    "\n",
    "The New York Times attempts to map their articles to a latent space of topics using the content of the article.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"assets/images/nyt_lda.png\" style=\"width:60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimensionality reduction in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our previous ‘representation’ of a set of text documents (articles) for classification was a matrix with one row per document and one column per word (or n-gram).\n",
    "\n",
    "While this sums up most of the information, it does drop a few things, mostly structure and order. \n",
    "\n",
    "Additionally, many of the columns may be correlated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, an article that contains the word `‘IPO’` is likely to contain the word `‘stock’` or `‘NASDAQ’`.\n",
    "\n",
    "Therefore, those columns are repetitive and likely to represent the same concept or idea.\n",
    "\n",
    "For classification, we may only care that there are finance-related words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To reduce this, we perform dimensionality reduction, where we first identify the correlated columns and the replace them with a column that represents the concept they have in common.\n",
    "\n",
    "For instance, we could replace `‘IPO’`, `‘stocks’`, and `‘NASDAQ’` with a single column - `‘HasFinancialWords’` column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are many techniques to do this automatically and most follow a very similar approach.\n",
    "\n",
    "- Identify correlated columns.\n",
    "\n",
    "- Replace them with a new column that encapsulates the others.\n",
    "\n",
    "<img src=\"assets/images/correlated_reduction.png\" style=\"width:60%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Mixture models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mixture models (specifically LDA or **Latent Dirichlet Allocation**) take this concept further and generate more structure around the documents.\n",
    "\n",
    "Instead of just replacing correlated columns, we create clusters of common words and generate probability distributions to explicitly state how related words are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This ‘model’ of text is assuming that each document is some mixture of topics.\n",
    "\n",
    "It may be mostly science but may contain some business or news information.\n",
    "\n",
    "The latent structure we want to uncover are the topics (or concepts) that generate that text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Latent Dirichlet Allocation is a model that assumes this is the way text is generated and then attempts to learn two things:\n",
    "\n",
    "- The word distribution of each topic\n",
    "\n",
    "- The topic distribution of each document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The word distribution is a multinomial distribution of each topic representing what words are most likely from that topic.\n",
    "\n",
    "<img src=\"assets/images/word_dist.png\" style=\"width:60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, let’s say we have three topics: sports, business, and science.\n",
    "\n",
    "For each topic, we uncover the most likely words to come from them:\n",
    "\n",
    "- sports: [football: 0.3, basketball: 0.2, baseball: 0.2, touchdown: 0.02 ... genetics: 0.0001]\n",
    "- science: [genetics: 0.2, drug: 0.2, ... baseball: 0.0001]\n",
    "- business: [stocks: 0.1, ipo: 0.08,  ... baseball: 0.0001]\n",
    "\n",
    "For each word and topic pair, we learn some probability:\n",
    "                          **P(word|topic)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The topic distribution is a multinomial distribution for each document representing what topics are most likely to appear in that document.\n",
    "\n",
    "For all our of sample documents, we have a distribution over {sports, science, business}.\n",
    "\n",
    "- ESPN article: [sports: 0.8, business: 0.2, science: 0.0]\n",
    "- Bloomberg article: [business: 0.7, science: 0.2, sports: 0.1]\n",
    "\n",
    "For each topic and document pair, we learn some probability: **P(topic|document)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Topic models are useful for organizing a collection of documents and uncovering the main underlying concepts.\n",
    "\n",
    "There are many variants that attempt to add even more structure to the ‘model’:\n",
    "\n",
    "- Supervised topic models guide the process with pre-decided topics.\n",
    "\n",
    "- Position-dependent topic models ignore which words occur in which document and instead focus on where they occur.\n",
    "\n",
    "- Variable number topic models test different numbers of topics to find the best model.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
