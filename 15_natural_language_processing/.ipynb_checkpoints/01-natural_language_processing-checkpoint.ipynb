{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Natural Language Processing\n",
    " \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Housekeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- UP3 should be submitted. Grades to be returned to you ASAP.\n",
    "\n",
    "\n",
    "\n",
    "- FP3 to be submitted by 6pm on Wednesday. **Only via Github**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Is Natural Language Processing (NLP)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- Using computers to process (analyze, understand, generate) natural human languages.\n",
    "\n",
    "\n",
    "- Making sense of human knowledge stored as unstructured text.\n",
    "\n",
    "\n",
    "- Building probabilistic models using data about a language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What Are Some of the Higher-Level Task Areas?\n",
    "\n",
    "\n",
    "We often hope that computers can solve many high-level problems involving natural language. Unfortunately, due to the difficulty of understanding human language, many of these problems are still not well solved. That said, existing solutions to these problems all involve utilizing the lower-level components of NLP discussed in the next section. \n",
    "\n",
    "Some higher-level tasks include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Chatbots:** Understand natural language from the user and return intelligent responses.\n",
    "    - [Api.ai](https://api.ai/)\n",
    "    \n",
    "    \n",
    "- **Information retrieval:** Find relevant results and similar results.\n",
    "    - [Google](https://www.google.com/)    \n",
    "    \n",
    "    \n",
    "- **Information extraction:** Structured information from unstructured documents.\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Machine translation:** One language to another.\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "    \n",
    "    \n",
    "- **Text simplification:** Preserve the meaning of text, but simplify the grammar and vocabulary.\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "    \n",
    "    \n",
    "- **Predictive text input:** Faster or easier typing.\n",
    "    - [Phrase completion application](https://justmarkham.shinyapps.io/textprediction/)\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Sentiment analysis:** Attitude of speaker.\n",
    "    - [Hater News](https://medium.com/@KevinMcAlear/building-hater-news-62062c58325c)\n",
    "    \n",
    "    \n",
    "- **Automatic summarization:** Extractive or abstractive summarization.\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "    \n",
    "    \n",
    "- **Natural language generation:** Generate text from data.\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Speech recognition and generation:** Speech-to-text, text-to-speech.\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "       \n",
    "       \n",
    "- **Question answering:** Determine the intent of the question, match query with knowledge base, evaluate hypotheses.\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What Are Some of the Lower-Level Components?\n",
    "\n",
    "Unfortunately, the NLP programming libraries typically do not provide direct solutions for the high-level tasks above. Instead, they provide low-level building blocks that enable us to craft our own solutions. These include:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Tokenization:** Breaking text into tokens (words, sentences, n-grams)\n",
    "    \n",
    "    \n",
    "**Stop-word removal:** a/an/the\n",
    "    \n",
    "    \n",
    "**Stemming and lemmatization:** root word\n",
    "    \n",
    "    \n",
    "**TF-IDF:** word importance\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Part-of-speech tagging:** noun/verb/adjective\n",
    "\n",
    "    \n",
    "**Named entity recognition:** person/organization/location\n",
    "    \n",
    "    \n",
    "**Spelling correction:** \"New Yrok City\"\n",
    "    \n",
    "    \n",
    "**Word sense disambiguation:** \"buy a mouse\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Segmentation:** \"New York City subway\"\n",
    "    \n",
    "    \n",
    "**Language detection:** \"translate this page\"\n",
    "    \n",
    "    \n",
    "**Machine learning:** specialized models that work well with text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How Do We Use NLP in Data Science?\n",
    "\n",
    "In data science, we are often asked to analyze unstructured text or make a predictive model using it. Unfortunately, most data science techniques require numeric data. NLP libraries provide a tool set of methods to convert unstructured text into meaningful numeric data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Analysis:** NLP techniques provide tools to allow us to understand and analyze large amounts of text. For example:\n",
    "\n",
    "   - Analyze the positivity/negativity of comments on different websites. \n",
    "    \n",
    "    \n",
    "   - Extract key words from meeting notes and visualize how meeting topics change over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Vectorizing for machine learning:** When building a machine learning model, we typically must transform our data into numeric features. This process of transforming non-numeric data such as natural language into numeric features is called vectorization. For example:\n",
    "\n",
    "\n",
    "- Understanding related words. Using stemming, NLP lets us know that \"swim\", \"swims\", and \"swimming\" all refer to the same base word. This allows us to reduce the number of features used in our model.\n",
    "\n",
    "\n",
    "- Identifying important and unique words. Using TF-IDF (term frequency-inverse document frequency), we can identify which words are most likely to be meaningful in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "Natural language processing requires an understanding of the language and the world. Several limitations of NLP are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Ambiguity**:\n",
    "\n",
    "   - Hospitals Are Sued by 7 Foot Doctors\n",
    "   - Juvenile Court to Try Shooting Defendant\n",
    "   - Local High School Dropouts Cut in Half\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Non-standard English:** text messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Idioms:** \"throw in the towel\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Newly coined words:** \"retweet\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Tricky entity names:** \"Where is A Bug's Life playing?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLP using Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will use Yelp reviews to practice and discover common low-level NLP techniques.\n",
    "\n",
    "You should be familiar with these terms, as they are frequently used in NLP:\n",
    "- **corpus**: a collection of documents (derived from the Latin word for \"body\")\n",
    "- **corpora**: plural form of corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB         # Naive Bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Read yelp.csv into a DataFrame.\n",
    "\n",
    "yelp = pd.read_csv(\"assets/data/yelp.csv\")\n",
    "\n",
    "# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# Define X and y.\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# Split the new DataFrame into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"assets/images/yelp_1.png\" style=\"width:120%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **What:** Converts each document into a set of words and their counts.\n",
    "\n",
    "\n",
    "- **Why:** To use a machine learning model, we must convert unstructured text into numeric features.\n",
    "\n",
    "\n",
    "     - **Notes:** Relatively easy with English language text, not as easy with some languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Use CountVectorizer to create document-term matrices from X_train and X_test.\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16825)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rows are documents, columns are terms (aka \"tokens\" or \"features\", individual words in this situation).\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yyyyy', 'z11', 'za', 'zabba', 'zach', 'zam', 'zanella', 'zankou', 'zappos', 'zatsiki', 'zen', 'zero', 'zest', 'zexperience', 'zha', 'zhou', 'zia', 'zihuatenejo', 'zilch', 'zin', 'zinburger', 'zinburgergeist', 'zinc', 'zinfandel', 'zing', 'zip', 'zipcar', 'zipper', 'zippers', 'zipps', 'ziti', 'zoe', 'zombi', 'zombies', 'zone', 'zones', 'zoning', 'zoo', 'zoyo', 'zucca', 'zucchini', 'zuchinni', 'zumba', 'zupa', 'zuzu', 'zwiebel', 'zzed', 'éclairs', 'école', 'ém']\n"
     ]
    }
   ],
   "source": [
    "# Last 50 features\n",
    "print((vect.get_feature_names()[-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show vectorizer options.\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One common method of reducing the number of features is converting all text to lowercase before generating features! \n",
    "\n",
    "Note that to a computer, `aPPle` is a different token/\"word\" than `apple`. \n",
    "\n",
    "\n",
    "So, by converting both to lowercase letters, it ensures fewer features will be generated. It might be useful not to convert them to lowercase if capitalization matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zoning',\n",
       " 'zoo',\n",
       " 'zucchini',\n",
       " 'zuchinni',\n",
       " 'zupa',\n",
       " 'zwiebel',\n",
       " 'zzed',\n",
       " 'École',\n",
       " 'éclairs',\n",
       " 'ém']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't convert to lowercase.\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape\n",
    "vect.get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![DTM](assets/images/DTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using CountVectorizer in a Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that text classification is done in the same way as all other classification models. \n",
    "\n",
    "First, the text is vectorized into a set of numeric features - the CountVectorizer. \n",
    "\n",
    "Then, a standard machine learning classifier is applied. NLP libraries often include vectorizers and ML models that work particularly well with text.\n",
    "\n",
    "> We will refer to each piece of text we are trying to classify as a document.\n",
    "> - For example, a document could refer to an email, book chapter, tweet, article, or text message.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Text classification is the task of predicting which category or topic a text sample is from.**\n",
    "\n",
    "We may want to identify:\n",
    "- Is an article a sports or business story?\n",
    "- Does an email have positive or negative sentiment?\n",
    "- Is the rating of a recipe 1, 2, 3, 4, or 5 stars?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Predictions are often made by using the words as features and the label as the target output.**\n",
    "\n",
    "Starting out, we will make each unique word (across all documents) a single feature. In any given corpora, we may have hundreds of thousands of unique words, so we may have hundreds of thousands of features!\n",
    "\n",
    "- For a given document, the numeric value of each feature could be the number of times the word appears in the document.\n",
    "    - So, most features will have a value of zero, resulting in a sparse matrix of features.\n",
    "\n",
    "\n",
    "- This technique for vectorizing text is referred to as a **bag-of-words model**. \n",
    "    - It is called bag of words because the document's structure is lost — as if the words are all jumbled up in a bag. \n",
    "    - The first step to creating a bag-of-words model is to create a vocabulary of all possible words in the corpora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**We need to consider several things to decide if bag-of-words is appropriate.**\n",
    "\n",
    "- Does order of words matter?\n",
    "- Does punctuation matter?\n",
    "- Does upper or lower case matter?\n",
    "\n",
    "\n",
    "In this example, we will use a model very popular for text classification called Naive Bayes (the \"NB\" in `BinonmialNB` and `MultinomialNB` below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9187866927592955\n"
     ]
    }
   ],
   "source": [
    "# Use default options for CountVectorizer.\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Create document-term matrices.\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# Use Naive Bayes to predict the star rating.\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# Calculate accuracy.\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    838\n",
       "1    184\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent 5 Stars: 0.8199608610567515\n",
      "Percent 1 Stars: 0.18003913894324852\n"
     ]
    }
   ],
   "source": [
    "# Calculate null accuracy.\n",
    "y_test_binary = np.where(y_test==5, 1, 0) # five stars become 1, one stars become 0\n",
    "print('Percent 5 Stars:', y_test_binary.mean())\n",
    "print('Percent 1 Stars:', 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our model predicted ~92% accuracy, which is an improvement over this baseline 82% accuracy (assuming our model always predicts 5 stars).\n",
    "\n",
    "Let's look more into how the vectorizer works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " Notice how the data was transformed into this sparse matrix with 1,022 datapoints and 16,825 features!\n",
    "   - Recall that vectorizations of text will be mostly zeros, since only a few unique words are in each document.\n",
    "   - We have 3064 Yelp reviews in our training set.\n",
    "   - 16,825 unique words were found across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3064x16825 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 237720 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take a look at the vocabulary that was generated, containing 16,825 unique words.\n",
    "\n",
    "'vocabulary_' is a dictionary that converts each word to its index in the sparse matrix.\n",
    "   - For example, the word \"cheap\" is index #2789 in the sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filly': 5773,\n",
       " 'only': 10362,\n",
       " 'reviews': 12465,\n",
       " 'nine': 10069,\n",
       " 'now': 10180,\n",
       " 'wow': 16612,\n",
       " 'do': 4631,\n",
       " 'miss': 9578,\n",
       " 'this': 15093,\n",
       " 'place': 11186,\n",
       " '24hrs': 136,\n",
       " 'drive': 4809,\n",
       " 'thru': 15136,\n",
       " 'or': 10413,\n",
       " 'walk': 16195,\n",
       " 'up': 15834,\n",
       " 'ridiculously': 12514,\n",
       " 'cheap': 2789,\n",
       " 'tasty': 14838,\n",
       " 'of': 10286,\n",
       " 'course': 3679,\n",
       " 'the': 15032,\n",
       " 'arizona': 1018,\n",
       " 'burritos': 2286,\n",
       " 'are': 1003,\n",
       " 'good': 6571,\n",
       " 'everything': 5342,\n",
       " 'is': 7956,\n",
       " 'used': 15885,\n",
       " 'to': 15228,\n",
       " 'love': 8899,\n",
       " 'one': 10354,\n",
       " 'combos': 3233,\n",
       " 'you': 16727,\n",
       " 'get': 6433,\n",
       " 'beef': 1564,\n",
       " 'burrito': 2285,\n",
       " 'taco': 14720,\n",
       " 'rice': 12489,\n",
       " 'and': 805,\n",
       " 'beans': 1528,\n",
       " 'for': 6028,\n",
       " 'under': 15683,\n",
       " 'color': 3213,\n",
       " 'me': 9301,\n",
       " 'silly': 13462,\n",
       " 'call': 2398,\n",
       " 'sally': 12808,\n",
       " 'they': 15067,\n",
       " 'have': 7023,\n",
       " 'bomb': 1902,\n",
       " 'horchata': 7345,\n",
       " 'too': 15281,\n",
       " 'really': 12038,\n",
       " 'fresh': 6154,\n",
       " 'flautas': 5882,\n",
       " 'rolled': 12622,\n",
       " 'tacos': 14721,\n",
       " 'breakfast': 2069,\n",
       " 'damn': 3999,\n",
       " 'here': 7149,\n",
       " 'whether': 16394,\n",
       " 'drunk': 4837,\n",
       " 'sober': 13745,\n",
       " 'my': 9868,\n",
       " 'husband': 7476,\n",
       " 'absolutely': 353,\n",
       " 'restaurant': 12410,\n",
       " 'anytime': 889,\n",
       " 'find': 5790,\n",
       " 'myself': 9871,\n",
       " 'craving': 3757,\n",
       " 'mexican': 9440,\n",
       " 'food': 6006,\n",
       " 'first': 5828,\n",
       " 'that': 15027,\n",
       " 'pops': 11347,\n",
       " 'in': 7619,\n",
       " 'head': 7042,\n",
       " 'salsa': 12814,\n",
       " 'blanca': 1780,\n",
       " 'we': 16306,\n",
       " 'always': 735,\n",
       " 'encountered': 5144,\n",
       " 'friendly': 6171,\n",
       " 'welcoming': 16355,\n",
       " 'staff': 14107,\n",
       " 'amazing': 752,\n",
       " 'fulfilling': 6251,\n",
       " 'what': 16377,\n",
       " 'more': 9735,\n",
       " 'could': 3655,\n",
       " 'ask': 1101,\n",
       " 'went': 16361,\n",
       " 'today': 15237,\n",
       " 'after': 563,\n",
       " 'lunch': 8945,\n",
       " 'got': 6605,\n",
       " 'usual': 15893,\n",
       " 'lime': 8707,\n",
       " 'basil': 1477,\n",
       " 'real': 12029,\n",
       " 'mint': 9540,\n",
       " 'chip': 2896,\n",
       " 'which': 16396,\n",
       " 'leaves': 8570,\n",
       " 'hubby': 7420,\n",
       " 'chocolate': 2914,\n",
       " 'guiness': 6820,\n",
       " 'four': 6089,\n",
       " 'peaks': 10866,\n",
       " 'hop': 7329,\n",
       " 'knot': 8348,\n",
       " 'best': 1657,\n",
       " 'ice': 7503,\n",
       " 'cream': 3766,\n",
       " 'phoenix': 11045,\n",
       " 'super': 14545,\n",
       " 'nice': 10041,\n",
       " 'give': 6481,\n",
       " 'us': 15879,\n",
       " 'bags': 1345,\n",
       " 'take': 14743,\n",
       " 'our': 10493,\n",
       " 'home': 7283,\n",
       " 'totally': 15327,\n",
       " 'dissapointed': 4585,\n",
       " 'had': 6868,\n",
       " 'purchased': 11785,\n",
       " 'coupon': 3676,\n",
       " 'from': 6195,\n",
       " 'travelzoo': 15435,\n",
       " 'try': 15520,\n",
       " 'out': 10496,\n",
       " 'given': 6483,\n",
       " 'its': 7980,\n",
       " 'location': 8814,\n",
       " 'would': 16606,\n",
       " 'thought': 15104,\n",
       " 'it': 7971,\n",
       " 'was': 16252,\n",
       " 'going': 6551,\n",
       " 'be': 1522,\n",
       " 'very': 16032,\n",
       " 'upscale': 15861,\n",
       " 'were': 16362,\n",
       " 'expecting': 5440,\n",
       " 'whole': 16429,\n",
       " 'lot': 8881,\n",
       " 'service': 13193,\n",
       " 'great': 6693,\n",
       " 'but': 2312,\n",
       " 'not': 10148,\n",
       " 'worth': 16601,\n",
       " 'itself': 7981,\n",
       " 'outdated': 10500,\n",
       " '80': 284,\n",
       " 'crab': 3721,\n",
       " 'cakes': 2387,\n",
       " 'appertizer': 933,\n",
       " 'cold': 3192,\n",
       " 'when': 16389,\n",
       " 'served': 13188,\n",
       " 'told': 15254,\n",
       " 'waiter': 16181,\n",
       " 'who': 16425,\n",
       " 'turn': 15556,\n",
       " 'chef': 2828,\n",
       " 'message': 9416,\n",
       " 'passed': 10776,\n",
       " 'back': 1310,\n",
       " 'she': 13272,\n",
       " 'appologized': 957,\n",
       " 'ordred': 10434,\n",
       " '12oz': 52,\n",
       " 'new': 10013,\n",
       " 'york': 16724,\n",
       " 'strip': 14358,\n",
       " 'ala': 646,\n",
       " 'carte': 2573,\n",
       " '29': 153,\n",
       " '95': 307,\n",
       " 'oz': 10602,\n",
       " 'pure': 11789,\n",
       " 'fat': 5622,\n",
       " 'price': 11555,\n",
       " 'expect': 5435,\n",
       " 'meat': 9318,\n",
       " 'than': 15015,\n",
       " 'ordered': 10428,\n",
       " 'wild': 16461,\n",
       " 'mushroom': 9844,\n",
       " 'pizza': 11174,\n",
       " 'ok': 10322,\n",
       " 'needs': 9972,\n",
       " 'an': 797,\n",
       " 'over': 10532,\n",
       " 'haul': 7014,\n",
       " 'major': 9048,\n",
       " 'way': 16300,\n",
       " 'if': 7534,\n",
       " 'want': 16220,\n",
       " 'make': 9051,\n",
       " 'any': 879,\n",
       " 'money': 9689,\n",
       " 'at': 1157,\n",
       " 'saturday': 12905,\n",
       " 'night': 10052,\n",
       " '7pm': 282,\n",
       " 'empty': 5125,\n",
       " 'think': 15082,\n",
       " 'highlight': 7186,\n",
       " 'meal': 9303,\n",
       " 'bottle': 1981,\n",
       " 'cabinet': 2365,\n",
       " 'costco': 3635,\n",
       " 'travel': 15428,\n",
       " 'recently': 12073,\n",
       " 'returned': 12444,\n",
       " 'trip': 15483,\n",
       " 'big': 1704,\n",
       " 'island': 7958,\n",
       " 'hi': 7168,\n",
       " 'arranged': 1038,\n",
       " 'throught': 15130,\n",
       " 'shopping': 13361,\n",
       " 'around': 1033,\n",
       " 'found': 6083,\n",
       " 'their': 15039,\n",
       " 'prices': 11559,\n",
       " 'phone': 11049,\n",
       " 'able': 340,\n",
       " 'all': 679,\n",
       " 'arrangements': 1040,\n",
       " 'airfair': 621,\n",
       " 'condo': 3394,\n",
       " 'car': 2505,\n",
       " 'didn': 4411,\n",
       " 'with': 16526,\n",
       " 'bs': 2197,\n",
       " 'on': 10352,\n",
       " 'hilo': 7206,\n",
       " 'adjust': 485,\n",
       " 'dates': 4053,\n",
       " 'according': 398,\n",
       " 'plan': 11196,\n",
       " 'being': 1599,\n",
       " 'accurate': 405,\n",
       " 'outstanding': 10527,\n",
       " 'value': 15939,\n",
       " 'gas': 6356,\n",
       " 'did': 4409,\n",
       " 'some': 13798,\n",
       " 'souvenier': 13885,\n",
       " 'kona': 8363,\n",
       " 'again': 573,\n",
       " 'saved': 12923,\n",
       " 've': 15968,\n",
       " 'shopped': 13358,\n",
       " 'years': 16679,\n",
       " 'becoming': 1555,\n",
       " 'groupie': 6766,\n",
       " 'been': 1567,\n",
       " 'couple': 3672,\n",
       " 'there': 15057,\n",
       " 'advantages': 520,\n",
       " 'rarely': 11967,\n",
       " 'busy': 2311,\n",
       " 'larger': 8478,\n",
       " 'groups': 6770,\n",
       " 'last': 8490,\n",
       " 'few': 5720,\n",
       " 'times': 15190,\n",
       " 'experienced': 5448,\n",
       " 'average': 1253,\n",
       " 'lousy': 8897,\n",
       " 'joining': 8119,\n",
       " 'already': 719,\n",
       " 'seated': 13085,\n",
       " 'group': 6765,\n",
       " '28': 152,\n",
       " 'high': 7181,\n",
       " 'school': 12990,\n",
       " 'hostess': 7369,\n",
       " 'wagged': 16174,\n",
       " 'her': 7142,\n",
       " 'finger': 5800,\n",
       " 'face': 5528,\n",
       " 'like': 8695,\n",
       " 'east': 4943,\n",
       " 'german': 6428,\n",
       " 'border': 1953,\n",
       " 'guard': 6800,\n",
       " 'waffle': 16169,\n",
       " 'house': 7390,\n",
       " 'waitress': 16184,\n",
       " 'poked': 11295,\n",
       " 'shoulder': 13376,\n",
       " 'attention': 1197,\n",
       " 'wife': 16449,\n",
       " 'chicken': 2855,\n",
       " 'dish': 4540,\n",
       " 'sent': 13162,\n",
       " 'as': 1080,\n",
       " 'measly': 9314,\n",
       " 'pieces': 11099,\n",
       " 'beers': 1571,\n",
       " 'tap': 14792,\n",
       " 'garbage': 6337,\n",
       " 'muffler': 9815,\n",
       " 'shop': 13354,\n",
       " 'town': 15357,\n",
       " 'shops': 13362,\n",
       " 'trust': 15513,\n",
       " 'mighty': 9482,\n",
       " 'them': 15042,\n",
       " 'greg': 6714,\n",
       " 'has': 6999,\n",
       " 'worked': 16580,\n",
       " 'multiple': 9824,\n",
       " 'cars': 2571,\n",
       " 'done': 4687,\n",
       " 'job': 8105,\n",
       " 'ever': 5333,\n",
       " 'issues': 7968,\n",
       " 'he': 7041,\n",
       " 'takes': 14748,\n",
       " 'care': 2525,\n",
       " 'problem': 11605,\n",
       " 'no': 10084,\n",
       " 'questions': 11856,\n",
       " 'asked': 1102,\n",
       " 'just': 8189,\n",
       " 'start': 14159,\n",
       " 'off': 10287,\n",
       " 'by': 2346,\n",
       " 'saying': 12940,\n",
       " 'egg': 5023,\n",
       " 'salad': 12788,\n",
       " 'sandwiches': 12862,\n",
       " 'probably': 11603,\n",
       " 'tried': 15473,\n",
       " 'sandwich': 12860,\n",
       " 'anywhere': 892,\n",
       " 'serves': 13192,\n",
       " 'sacks': 12762,\n",
       " 'far': 5600,\n",
       " 'entitled': 5217,\n",
       " 'dali': 3991,\n",
       " 'life': 8673,\n",
       " 'live': 8775,\n",
       " 'north': 10136,\n",
       " 'will': 16470,\n",
       " 'literally': 8770,\n",
       " 'many': 9120,\n",
       " 'miles': 9493,\n",
       " 'eat': 4949,\n",
       " 'top': 15293,\n",
       " 'wonderful': 16554,\n",
       " 'menu': 9396,\n",
       " 'whenever': 16390,\n",
       " 'order': 10427,\n",
       " 'comes': 3237,\n",
       " 'little': 8773,\n",
       " 'cookie': 3558,\n",
       " 'cookies': 3559,\n",
       " 'can': 2435,\n",
       " 'purchase': 11784,\n",
       " 'dough': 4722,\n",
       " 'also': 721,\n",
       " 'other': 10485,\n",
       " 'delicious': 4220,\n",
       " 'dessert': 4342,\n",
       " 'bars': 1458,\n",
       " 'salads': 12789,\n",
       " 'sale': 12796,\n",
       " 'well': 16356,\n",
       " 'eating': 4958,\n",
       " 'least': 8566,\n",
       " '14': 56,\n",
       " 'hope': 7330,\n",
       " 'never': 10012,\n",
       " 'go': 6532,\n",
       " 'away': 1275,\n",
       " 'saw': 12934,\n",
       " 'triple': 15485,\n",
       " 'so': 13738,\n",
       " 'decided': 4131,\n",
       " 'expectations': 5438,\n",
       " 'arrived': 1048,\n",
       " 'shocked': 13339,\n",
       " 'thursday': 15145,\n",
       " 'morning': 9742,\n",
       " 'line': 8724,\n",
       " 'extra': 5494,\n",
       " 'long': 8842,\n",
       " 'wait': 16179,\n",
       " 'patiently': 10810,\n",
       " 'don': 4682,\n",
       " 'needless': 9971,\n",
       " 'say': 12938,\n",
       " 'because': 1548,\n",
       " 'quickly': 11864,\n",
       " 'pork': 11358,\n",
       " 'chop': 2938,\n",
       " 'eggs': 5030,\n",
       " 'hash': 7000,\n",
       " 'browns': 2168,\n",
       " 'fabulous': 5526,\n",
       " 'toast': 15229,\n",
       " 'made': 8998,\n",
       " 'grape': 6659,\n",
       " 'jelly': 8061,\n",
       " 'recommend': 12097,\n",
       " 'anyone': 885,\n",
       " 'downtown': 4743,\n",
       " 'area': 1004,\n",
       " 'sure': 14583,\n",
       " 'bacon': 1324,\n",
       " 'hit': 7232,\n",
       " 'wine': 16492,\n",
       " 'down': 4732,\n",
       " 'wednesday': 16325,\n",
       " 'happenin': 6955,\n",
       " 'tastings': 14837,\n",
       " 'courtesy': 3685,\n",
       " 'kyot': 8401,\n",
       " 'wrong': 16639,\n",
       " 'event': 5330,\n",
       " 'minutes': 9546,\n",
       " 'maybe': 9272,\n",
       " 'spot': 14035,\n",
       " 'case': 2583,\n",
       " 'hoppin': 7339,\n",
       " 'walked': 16197,\n",
       " 'looks': 8855,\n",
       " 'possibly': 11391,\n",
       " 'manager': 9088,\n",
       " 'owner': 10590,\n",
       " 'then': 15047,\n",
       " 'pointed': 11285,\n",
       " 'towards': 15350,\n",
       " 'room': 12641,\n",
       " 'where': 16391,\n",
       " 'held': 7114,\n",
       " 'about': 343,\n",
       " 'tables': 14711,\n",
       " 'people': 10927,\n",
       " 'talk': 14757,\n",
       " 'early': 4926,\n",
       " 'bird': 1735,\n",
       " 'gets': 6435,\n",
       " 'worm': 16591,\n",
       " 'finish': 5808,\n",
       " 'table': 14709,\n",
       " 'sit': 13510,\n",
       " 'patio': 10812,\n",
       " 'said': 12778,\n",
       " 'yes': 16698,\n",
       " 'right': 12522,\n",
       " 'help': 7124,\n",
       " 'meantime': 9312,\n",
       " 'another': 850,\n",
       " 'woman': 16548,\n",
       " 'friend': 6167,\n",
       " 'took': 15283,\n",
       " 'nearby': 9954,\n",
       " '20': 107,\n",
       " 'later': 8498,\n",
       " 'looking': 8853,\n",
       " 'came': 2417,\n",
       " 'helped': 7125,\n",
       " 'yet': 16701,\n",
       " 'left': 8583,\n",
       " 'return': 12443,\n",
       " 'small': 13638,\n",
       " 'happy': 6963,\n",
       " 'hour': 7388,\n",
       " 'flier': 5905,\n",
       " 'handed': 6914,\n",
       " 'distance': 4592,\n",
       " 'even': 5327,\n",
       " 'come': 3234,\n",
       " 'know': 8351,\n",
       " 're': 12013,\n",
       " 'wondering': 16557,\n",
       " 'why': 16440,\n",
       " '15': 60,\n",
       " 'wanted': 16222,\n",
       " 'see': 13113,\n",
       " 'how': 7404,\n",
       " 'bad': 1326,\n",
       " 'frankly': 6118,\n",
       " 'same': 12833,\n",
       " 'full': 6252,\n",
       " 'detailed': 4352,\n",
       " 'menus': 9398,\n",
       " 'deals': 4093,\n",
       " 'ready': 12028,\n",
       " 'um': 15635,\n",
       " 'gave': 6375,\n",
       " '10': 16,\n",
       " 'seconds': 13096,\n",
       " 'ago': 587,\n",
       " 'enjoy': 5179,\n",
       " 'look': 8851,\n",
       " 'app': 913,\n",
       " 'specialty': 13945,\n",
       " 'appetizer': 938,\n",
       " 'much': 9807,\n",
       " 'whatever': 16378,\n",
       " 'deal': 4087,\n",
       " 'drink': 4801,\n",
       " 'section': 13103,\n",
       " 'claim': 3029,\n",
       " 'glasses': 6497,\n",
       " 'excited': 5387,\n",
       " 'wines': 16493,\n",
       " 'reality': 12031,\n",
       " 'consisted': 3467,\n",
       " 'three': 15116,\n",
       " 'different': 4427,\n",
       " 'weren': 16363,\n",
       " 'women': 16549,\n",
       " 'outside': 10525,\n",
       " 'leave': 8569,\n",
       " 'having': 7026,\n",
       " 'enough': 5189,\n",
       " 'shenanigans': 13295,\n",
       " '30': 162,\n",
       " 'since': 13482,\n",
       " 'next': 10031,\n",
       " 'door': 4699,\n",
       " 'sprouts': 14064,\n",
       " 'apps': 978,\n",
       " 'own': 10588,\n",
       " 'bet': 1660,\n",
       " 'plenty': 11250,\n",
       " 'experiences': 5449,\n",
       " 'continue': 3514,\n",
       " 'visit': 16107,\n",
       " 'happily': 6961,\n",
       " 'experience': 5447,\n",
       " 'caused': 2628,\n",
       " 'write': 16632,\n",
       " 'usually': 15894,\n",
       " 'instance': 7807,\n",
       " 'wish': 16521,\n",
       " 'actually': 447,\n",
       " 'review': 12460,\n",
       " 'poor': 11337,\n",
       " 'substandard': 14446,\n",
       " 'giving': 6486,\n",
       " 'understand': 15696,\n",
       " 'isn': 7962,\n",
       " 'better': 1667,\n",
       " 'talented': 14753,\n",
       " 'others': 10486,\n",
       " 'however': 7407,\n",
       " 'business': 2300,\n",
       " 'should': 13375,\n",
       " 'customer': 3946,\n",
       " 'cliched': 3082,\n",
       " 'less': 8626,\n",
       " 'star': 14146,\n",
       " 'oh': 10310,\n",
       " 'yeah': 16674,\n",
       " 'slapped': 13575,\n",
       " 'yelped': 16689,\n",
       " 'card': 2516,\n",
       " 'pepperoni': 10937,\n",
       " 'amazed': 749,\n",
       " 'herb': 7143,\n",
       " 'vegetable': 15976,\n",
       " 'garden': 6340,\n",
       " 'front': 6197,\n",
       " 'ingredients': 7747,\n",
       " 'these': 15065,\n",
       " 'days': 4067,\n",
       " 'dishes': 4541,\n",
       " 'cheese': 2818,\n",
       " 'platters': 11225,\n",
       " 'mouth': 9782,\n",
       " 'watering': 16285,\n",
       " 'speak': 13926,\n",
       " 'day': 4064,\n",
       " 'll': 8788,\n",
       " 'keep': 8244,\n",
       " 'adding': 469,\n",
       " 'stay': 14186,\n",
       " 'tuned': 15547,\n",
       " 'btw': 2198,\n",
       " 'decor': 4149,\n",
       " 'reminds': 12272,\n",
       " 'vig': 16066,\n",
       " 'definitely': 4188,\n",
       " 'spacious': 13898,\n",
       " 'dining': 4462,\n",
       " 'sushi': 14612,\n",
       " 'time': 15184,\n",
       " 'garlic': 6346,\n",
       " 'knots': 8350,\n",
       " 'favorite': 5643,\n",
       " 'alot': 716,\n",
       " 'panda': 10687,\n",
       " 'family': 5581,\n",
       " 'run': 12728,\n",
       " 'chinese': 2894,\n",
       " 'most': 9757,\n",
       " 'importantly': 7596,\n",
       " 'style': 14421,\n",
       " 'meals': 9304,\n",
       " 'personal': 10987,\n",
       " 'empress': 5122,\n",
       " 'sweeter': 14655,\n",
       " 'orange': 10415,\n",
       " 'flavor': 5884,\n",
       " 'melts': 9371,\n",
       " 'your': 16732,\n",
       " 'kung': 8396,\n",
       " 'pao': 10705,\n",
       " 'two': 15598,\n",
       " 'shrimp': 13403,\n",
       " 'leftovers': 8585,\n",
       " 'thanks': 15024,\n",
       " 'generous': 6407,\n",
       " 'portions': 11369,\n",
       " 'selection': 13131,\n",
       " 'sake': 12784,\n",
       " 'beer': 1570,\n",
       " 'everyone': 5340,\n",
       " 'else': 5078,\n",
       " 'checks': 2809,\n",
       " 'man': 9083,\n",
       " 'leaving': 8571,\n",
       " 'six': 13519,\n",
       " 'half': 6889,\n",
       " 'search': 13073,\n",
       " 'gotten': 6610,\n",
       " 'point': 11283,\n",
       " 'settled': 13205,\n",
       " 'decent': 4125,\n",
       " 'work': 16579,\n",
       " 'colleague': 3202,\n",
       " 'suggested': 14496,\n",
       " 'dine': 4454,\n",
       " 'corporate': 3610,\n",
       " 'meeting': 9348,\n",
       " 'while': 16398,\n",
       " 'realized': 12033,\n",
       " 'native': 9933,\n",
       " 'metro': 9435,\n",
       " 'thing': 15077,\n",
       " 'trusted': 15514,\n",
       " 'his': 7227,\n",
       " 'palate': 10656,\n",
       " 'sense': 13156,\n",
       " 'pleased': 11243,\n",
       " 'diners': 4458,\n",
       " 'predominately': 11467,\n",
       " 'asian': 1098,\n",
       " 'knew': 8337,\n",
       " 'market': 9165,\n",
       " 'things': 15079,\n",
       " 'signaled': 13445,\n",
       " 'thrilled': 15123,\n",
       " 'familiar': 5579,\n",
       " 'dim': 4448,\n",
       " 'sum': 14510,\n",
       " 'carts': 2578,\n",
       " 'rolling': 12624,\n",
       " 'through': 15128,\n",
       " 'party': 10768,\n",
       " 'shared': 13254,\n",
       " 'fantastic': 5596,\n",
       " 'succulent': 14469,\n",
       " 'orders': 10431,\n",
       " 'equally': 5244,\n",
       " 'veggies': 15981,\n",
       " 'brightly': 2120,\n",
       " 'colored': 3215,\n",
       " 'tastes': 14830,\n",
       " 'distinctive': 4598,\n",
       " 'hot': 7374,\n",
       " 'sour': 13871,\n",
       " 'soup': 13868,\n",
       " 'perfect': 10949,\n",
       " 'filled': 5766,\n",
       " 'kinds': 8310,\n",
       " 'mushrooms': 9845,\n",
       " 'etc': 5301,\n",
       " 'delighted': 4226,\n",
       " 'reminisced': 12273,\n",
       " 'dinners': 4466,\n",
       " 'chinatown': 2893,\n",
       " 'ny': 10224,\n",
       " 'comparable': 3290,\n",
       " 'close': 3108,\n",
       " 'regular': 12197,\n",
       " 'chandler': 2741,\n",
       " 'clear': 3067,\n",
       " 'side': 13428,\n",
       " 'world': 16588,\n",
       " 'figure': 5750,\n",
       " 'marco': 9133,\n",
       " 'polo': 11314,\n",
       " 'palace': 10653,\n",
       " 'prompt': 11671,\n",
       " 'rounded': 12678,\n",
       " 'hooray': 7326,\n",
       " 'stayed': 14189,\n",
       " 'hotel': 7377,\n",
       " 'tuscany': 15566,\n",
       " 'scallops': 12951,\n",
       " 'pasta': 10788,\n",
       " 'excellent': 5374,\n",
       " 'server': 13189,\n",
       " 'bliss': 1820,\n",
       " 'haven': 7024,\n",
       " 'quality': 11833,\n",
       " 'dinner': 4465,\n",
       " 'stupendous': 14413,\n",
       " 'every': 5336,\n",
       " 'cent': 2674,\n",
       " 'must': 9856,\n",
       " 'yin': 16705,\n",
       " 'yang': 16663,\n",
       " 'martini': 9195,\n",
       " 'creative': 3779,\n",
       " 'presented': 11514,\n",
       " 'smiles': 13664,\n",
       " 'smores': 13684,\n",
       " 'desert': 4315,\n",
       " 'hands': 6928,\n",
       " 'connosiuer': 3444,\n",
       " 'critical': 3821,\n",
       " 'college': 3209,\n",
       " 'kids': 8288,\n",
       " 'ra': 11887,\n",
       " 'mediocre': 9341,\n",
       " 'list': 8755,\n",
       " 'irk': 7938,\n",
       " 'during': 4891,\n",
       " 'co': 3145,\n",
       " 'workers': 16582,\n",
       " 'packed': 10616,\n",
       " 'extremely': 5503,\n",
       " 'meet': 9347,\n",
       " 'coworkers': 3708,\n",
       " 'tired': 15214,\n",
       " 'chill': 2878,\n",
       " 'past': 10787,\n",
       " 'friends': 6172,\n",
       " 'alone': 710,\n",
       " 'quite': 11877,\n",
       " 'bit': 1749,\n",
       " 'servings': 13197,\n",
       " 'obviously': 10256,\n",
       " 'low': 8911,\n",
       " 'zen': 16785,\n",
       " '32': 172,\n",
       " 'choice': 2922,\n",
       " 'sakebomber': 12785,\n",
       " 'loud': 8886,\n",
       " 'noises': 10098,\n",
       " 'trying': 15521,\n",
       " 'louder': 8887,\n",
       " 'music': 9848,\n",
       " 'person': 10985,\n",
       " 'overall': 10533,\n",
       " 'tip': 15207,\n",
       " 'priced': 11556,\n",
       " 'works': 16587,\n",
       " 'horrible': 7351,\n",
       " 'twice': 15586,\n",
       " 'gone': 6564,\n",
       " 'messing': 9420,\n",
       " 'starbucks': 14148,\n",
       " 'hard': 6969,\n",
       " 'believe': 1604,\n",
       " 'attitude': 1202,\n",
       " 'lady': 8427,\n",
       " 'doesn': 4648,\n",
       " 'ordering': 10429,\n",
       " 'someone': 13803,\n",
       " 'fix': 5846,\n",
       " 'known': 8356,\n",
       " 'ya': 16655,\n",
       " 'employees': 5117,\n",
       " 'rude': 12708,\n",
       " 'makes': 9055,\n",
       " 'anymore': 884,\n",
       " 'stopped': 14290,\n",
       " 'still': 14252,\n",
       " 'mom': 9679,\n",
       " 'goes': 6549,\n",
       " 'coffee': 3182,\n",
       " 'coast': 3151,\n",
       " 'deli': 4210,\n",
       " 'west': 16365,\n",
       " 'appalachians': 914,\n",
       " 'creams': 3771,\n",
       " 'pricy': 11566,\n",
       " 'apparently': 921,\n",
       " 'free': 6134,\n",
       " 'summer': 14515,\n",
       " 'kept': 8256,\n",
       " 'stars': 14158,\n",
       " 'frown': 6207,\n",
       " 'birthday': 1739,\n",
       " 'spirits': 13998,\n",
       " 'written': 16637,\n",
       " 'clarity': 3042,\n",
       " 'margaritas': 9140,\n",
       " 'strong': 14376,\n",
       " 'check': 2797,\n",
       " '18': 75,\n",
       " 'mahi': 9025,\n",
       " 'each': 4917,\n",
       " '24': 135,\n",
       " 'rancid': 11948,\n",
       " 'yup': 16765,\n",
       " 'fajitas': 5564,\n",
       " 'jumbo': 8173,\n",
       " 'charge': 2763,\n",
       " 'entree': 5222,\n",
       " 'pay': 10844,\n",
       " 'onions': 10360,\n",
       " 'once': 10353,\n",
       " 'week': 16329,\n",
       " 'asada': 1081,\n",
       " 'enchilada': 5138,\n",
       " 'avocado': 1260,\n",
       " 'lover': 8905,\n",
       " 'ripest': 12542,\n",
       " 'avocados': 1261,\n",
       " 'f724': 5521,\n",
       " 'bother': 1975,\n",
       " 'asking': 1104,\n",
       " 'entering': 5201,\n",
       " 'reply': 12331,\n",
       " 'spoke': 14020,\n",
       " 'acknowledged': 419,\n",
       " 'presence': 11511,\n",
       " 'ticket': 15153,\n",
       " 'happened': 6954,\n",
       " 'effort': 5020,\n",
       " 'tell': 14925,\n",
       " 'scathing': 12969,\n",
       " 'deplorable': 4288,\n",
       " 'sadly': 12768,\n",
       " 'dmv': 4628,\n",
       " 'modeled': 9641,\n",
       " 'agonizing': 588,\n",
       " 'depths': 4297,\n",
       " 'hell': 7117,\n",
       " 'musac': 9837,\n",
       " 'score': 13009,\n",
       " 'playing': 11233,\n",
       " 'voracious': 16143,\n",
       " 'stench': 14221,\n",
       " 'air': 619,\n",
       " 'spastic': 13921,\n",
       " 'children': 2869,\n",
       " 'running': 12731,\n",
       " 'version': 16028,\n",
       " 'politicians': 11310,\n",
       " 'campaigning': 2426,\n",
       " 'late': 8496,\n",
       " 'complete': 3331,\n",
       " 'emissions': 5104,\n",
       " 'test': 14990,\n",
       " 'those': 15101,\n",
       " 'dmvs': 4629,\n",
       " 'open': 10379,\n",
       " 'saturdays': 12906,\n",
       " 'map': 9121,\n",
       " 'precious': 11458,\n",
       " 'fortunate': 6071,\n",
       " 'near': 9953,\n",
       " 'seriously': 13180,\n",
       " 'entire': 5215,\n",
       " 'valley': 15936,\n",
       " 'receiving': 12071,\n",
       " 'such': 14471,\n",
       " 'magnificent': 9018,\n",
       " 'welcome': 16352,\n",
       " 'among': 781,\n",
       " 'painful': 10633,\n",
       " 'simplify': 13476,\n",
       " 'let': 8630,\n",
       " 'feel': 5678,\n",
       " 'constitutes': 3479,\n",
       " 'acceptable': 371,\n",
       " 'behavior': 1594,\n",
       " 'public': 11732,\n",
       " 'bathe': 1492,\n",
       " 'combination': 3227,\n",
       " 'water': 16280,\n",
       " 'soap': 13742,\n",
       " 'along': 711,\n",
       " 'massaging': 9221,\n",
       " 'skin': 13553,\n",
       " 'creates': 3775,\n",
       " 'outcome': 10499,\n",
       " 'pleasing': 11244,\n",
       " 'scrub': 13050,\n",
       " 'behind': 1595,\n",
       " 'ears': 4932,\n",
       " 'reference': 12142,\n",
       " 'website': 16318,\n",
       " 'notorious': 10164,\n",
       " 'http': 7416,\n",
       " 'www': 16645,\n",
       " 'craigslist': 3740,\n",
       " 'org': 10443,\n",
       " 'htf': 7414,\n",
       " '755891987': 278,\n",
       " 'html': 7415,\n",
       " 'waiting': 16183,\n",
       " 'tapping': 14800,\n",
       " 'foot': 6022,\n",
       " 'rocking': 12609,\n",
       " 'forth': 6070,\n",
       " 'chair': 2712,\n",
       " 'bouncing': 1999,\n",
       " 'leg': 8586,\n",
       " 'aren': 1006,\n",
       " 'faster': 5620,\n",
       " 'watching': 16279,\n",
       " 'clock': 3102,\n",
       " 'anxious': 877,\n",
       " 'leash': 8563,\n",
       " 'cannot': 2465,\n",
       " 'quietly': 11868,\n",
       " 'winter': 16506,\n",
       " 'fine': 5794,\n",
       " 'cute': 3953,\n",
       " 'monster': 9702,\n",
       " 'aisles': 634,\n",
       " 'honey': 7307,\n",
       " 'bun': 2261,\n",
       " 'squeezing': 14083,\n",
       " 'between': 1671,\n",
       " 'fingers': 5805,\n",
       " 'fake': 5566,\n",
       " 'falls': 5575,\n",
       " 'five': 5845,\n",
       " 'row': 12686,\n",
       " 'maintain': 9039,\n",
       " 'quiet': 11866,\n",
       " 'tones': 15272,\n",
       " 'ensure': 5197,\n",
       " 'appropriate': 973,\n",
       " 'conversations': 3541,\n",
       " 'taking': 14749,\n",
       " 'making': 9060,\n",
       " 'calls': 2404,\n",
       " 'sorry': 13851,\n",
       " 'stripping': 14366,\n",
       " 'popped': 11342,\n",
       " 'into': 7877,\n",
       " 'mind': 9515,\n",
       " 'failed': 5550,\n",
       " 'audition': 1218,\n",
       " 'le': 8539,\n",
       " 'girl': 6470,\n",
       " 'continued': 3515,\n",
       " 'conversation': 3539,\n",
       " 'woodworker': 16571,\n",
       " 'acknowledge': 418,\n",
       " 'humans': 7440,\n",
       " 'fact': 5540,\n",
       " 'sideways': 13435,\n",
       " 'disappear': 4490,\n",
       " 'quit': 11876,\n",
       " 'bumping': 2259,\n",
       " 'hitting': 7235,\n",
       " 'unless': 15770,\n",
       " 'flirting': 5917,\n",
       " 'need': 9966,\n",
       " 'level': 8636,\n",
       " 'brilliant': 2122,\n",
       " 'recipe': 12083,\n",
       " 'dashing': 4050,\n",
       " 'dreams': 4778,\n",
       " 'gutting': 6850,\n",
       " 'humanity': 7439,\n",
       " 'manage': 9084,\n",
       " 'painfully': 10634,\n",
       " 'lit': 8766,\n",
       " 'reeks': 12137,\n",
       " 'brother': 2158,\n",
       " 'old': 10328,\n",
       " 'gym': 6853,\n",
       " 'shoes': 13345,\n",
       " 'dad': 3979,\n",
       " 'dirty': 4486,\n",
       " 'underwear': 15703,\n",
       " 'award': 1271,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# We will use this function below for simplicity.\n",
    "\n",
    "# Define a function that accepts a vectorizer and calculates the accuracy.\n",
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print(('Features: ', X_train_dtm.shape[1]))\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print(('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 8783)\n",
      "('Accuracy: ', 0.9246575342465754)\n"
     ]
    }
   ],
   "source": [
    "# min_df ignores words that occur less than twice ('df' means \"document frequency\").\n",
    "vect = CountVectorizer(min_df=2, max_features=10000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Refining the input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "N-grams are features which consist of N consecutive words. This is useful because using the bag-of-words model, treating `data scientist` as a single feature has more meaning than having two independent features `data` and `scientist`!\n",
    "\n",
    "Example:\n",
    "```\n",
    "my cat is awesome\n",
    "Unigrams (1-grams): 'my', 'cat', 'is', 'awesome'\n",
    "Bigrams (2-grams): 'my cat', 'cat is', 'is awesome'\n",
    "Trigrams (3-grams): 'my cat is', 'cat is awesome'\n",
    "4-grams: 'my cat is awesome'\n",
    "```\n",
    "\n",
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "\n",
    "\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 169847)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams.\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can start to see how supplementing our features with n-grams can lead to more feature columns. \n",
    "\n",
    "When we produce n-grams from a document with $W$ words, we add an additional $(n-W+1)$ features (at most). \n",
    "\n",
    "\n",
    "That said, be careful — when we compute n-grams from an entire corpus, the number of _unique_ n-grams could be vastly higher than the number of _unique_ unigrams! This could cause an undesired feature explosion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Although we sometimes add important new features that have meaning such as `data scientist`, many of the new features will just be noise. \n",
    "\n",
    "So, particularly if we do not have much data, adding n-grams can actually decrease model performance. \n",
    "\n",
    "This is because if each n-gram is only present once or twice in the training set, we are effectively adding mostly noisy features to the mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zone out', 'zone when', 'zones', 'zones dolls', 'zoning', 'zoning issues', 'zoo', 'zoo and', 'zoo is', 'zoo not', 'zoo the', 'zoo ve', 'zoyo', 'zoyo for', 'zucca', 'zucca appetizer', 'zucchini', 'zucchini and', 'zucchini bread', 'zucchini broccoli', 'zucchini carrots', 'zucchini fries', 'zucchini pieces', 'zucchini strips', 'zucchini veal', 'zucchini very', 'zucchini with', 'zuchinni', 'zuchinni again', 'zuchinni the', 'zumba', 'zumba class', 'zumba or', 'zumba yogalates', 'zupa', 'zupa flavors', 'zuzu', 'zuzu in', 'zuzu is', 'zuzu the', 'zwiebel', 'zwiebel kräuter', 'zzed', 'zzed in', 'éclairs', 'éclairs napoleons', 'école', 'école lenôtre', 'ém', 'ém all']\n"
     ]
    }
   ],
   "source": [
    "# Last 50 features\n",
    "print((vect.get_feature_names()[-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stop-Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **What:** This process is used to remove common words that will likely appear in any text.\n",
    "\n",
    "\n",
    "- **Why:** Because common words exist in most documents, they likely only add noise to your model and should be removed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**What are stop words?**\n",
    "Stop words are some of the most common words in a language. \n",
    "\n",
    "They are used so that a sentence makes sense grammatically, such as prepositions and determiners, e.g., \"to,\" \"the,\" \"and.\" \n",
    "\n",
    "However, they are so commonly used that they are generally worthless for predicting the class of a document. Since \"a\" appears in spam and non-spam emails, for example, it would only contribute noise to our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example: \n",
    "\n",
    "> 1. Original sentence: \"The dog jumped over the fence\"  \n",
    "> 2. After stop-word removal: \"dog jumped over fence\"\n",
    "\n",
    "The fact that there is a fence and a dog jumped over it can be derived with or without stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### CountVectorizer option\n",
    "\n",
    "**stop_words:** string {`english`}, list, or None (default)\n",
    "- If `english`, a built-in stop word list for English is used.\n",
    "\n",
    "\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "\n",
    "\n",
    "- If None, no stop words will be used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 16528)\n",
      "('Accuracy: ', 0.9158512720156555)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': 'english',\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove English stop words.\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)\n",
    "vect.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'to', 'first', 'everyone', 'each', 'often', 'seemed', 'there', 'we', 'something', 'amount', 'becoming', 'across', 'least', 'seems', 'though', 'those', 'bottom', 'fifty', 'get', 'name', 'afterwards', 'by', 'besides', 'hundred', 'why', 'becomes', 'serious', 'that', 'wherein', 'cant', 'and', 'ltd', 'almost', 'from', 'ten', 'mostly', 'move', 'out', 'are', 'thence', 'describe', 'once', 'interest', 'empty', 'ever', 'whose', 'do', 'last', 'whatever', 'your', 'sometimes', 'un', 'see', 'beside', 'show', 'everything', 'anyhow', 'next', 'beyond', 'either', 'a', 'beforehand', 'fifteen', 'formerly', 'herself', 'itself', 'two', 'can', 'per', 'twelve', 'toward', 'whom', 'yet', 'co', 'please', 'eg', 'here', 'because', 'further', 'whither', 'enough', 'namely', 'no', 'in', 'together', 'fire', 'even', 'most', 'through', 'give', 'none', 'moreover', 'top', 'they', 'under', 'someone', 'inc', 'same', 'due', 'it', 'few', 'should', 'all', 'could', 'mine', 'both', 'nine', 'if', 'therein', 'whereby', 'never', 'others', 'onto', 'their', 'him', 'couldnt', 'hereupon', 'had', 'she', 'what', 'whenever', 'thus', 'been', 'while', 'too', 'former', 'some', 'how', 'well', 'hers', 'for', 'between', 'thereby', 'again', 'has', 'twenty', 'eight', 'elsewhere', 'sixty', 'done', 'everywhere', 'thereafter', 'its', 'our', 'or', 'three', 'themselves', 'thru', 'several', 'although', 'behind', 'indeed', 'also', 'always', 'made', 'meanwhile', 'less', 'hasnt', 'nobody', 'as', 'fill', 'her', 'became', 'sometime', 'such', 'ourselves', 'very', 'after', 'when', 'hereafter', 'were', 'during', 'into', 'against', 'hence', 'the', 'mill', 'ie', 'much', 'so', 'have', 'nothing', 'being', 'but', 'other', 'therefore', 'thin', 'an', 'cry', 'else', 'five', 'must', 'only', 'then', 'now', 'this', 'somehow', 'am', 'go', 'he', 'may', 'nor', 'myself', 'etc', 'seeming', 'third', 'whether', 'any', 'anywhere', 'his', 'six', 'anything', 'front', 'rather', 'still', 'below', 'eleven', 'latter', 'with', 'down', 'found', 'might', 'become', 'at', 'yourself', 'seem', 'de', 'another', 'nowhere', 'noone', 'otherwise', 'put', 'wherever', 'on', 'back', 'take', 'whole', 'detail', 'forty', 'yourselves', 'since', 'them', 'along', 'us', 'whoever', 'already', 'within', 'nevertheless', 'my', 'i', 'somewhere', 'be', 'anyone', 'every', 'than', 'these', 'cannot', 'thereupon', 'where', 'around', 'amongst', 'neither', 'amoungst', 'find', 'towards', 'until', 'which', 'yours', 'perhaps', 'call', 'one', 'ours', 'over', 'side', 'up', 'whereupon', 'who', 'latterly', 'sincere', 'hereby', 'anyway', 'thick', 'system', 'whereafter', 'throughout', 'whereas', 'will', 'alone', 'full', 'keep', 'me', 'more', 'part', 'among', 'off', 'bill', 'without', 'herein', 'via', 'you', 'four', 're', 'above', 'however', 'many', 'himself', 'whence', 'would', 'own', 'was', 'of', 'upon', 'before', 'is', 'about', 'con', 'not', 'except'})\n"
     ]
    }
   ],
   "source": [
    "# Set of stop words\n",
    "print((vect.get_stop_words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='cvec_opt'></a>\n",
    "### Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `max_features`: int or None, default=None\n",
    "    - If not `None`, build a vocabulary that only consider the top `max_features` ordered by term frequency across the corpus. This allows us to keep more common n-grams and remove ones that may appear once. If we include words that only occur once, this can lead to said features being highly associated with a class and cause overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 100)\n",
      "('Accuracy: ', 0.8698630136986302)\n"
     ]
    }
   ],
   "source": [
    "# Remove English stop words and only keep 100 features.\n",
    "vect = CountVectorizer(stop_words='english', max_features=100)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing', 'area', 'atmosphere', 'awesome', 'bad', 'bar', 'best', 'better', 'big', 'came', 'cheese', 'chicken', 'clean', 'coffee', 'come', 'day', 'definitely', 'delicious', 'did', 'didn', 'dinner', 'don', 'eat', 'excellent', 'experience', 'favorite', 'feel', 'food', 'free', 'fresh', 'friendly', 'friends', 'going', 'good', 'got', 'great', 'happy', 'home', 'hot', 'hour', 'just', 'know', 'like', 'little', 'll', 'location', 'long', 'looking', 'lot', 'love', 'lunch', 'make', 'meal', 'menu', 'minutes', 'need', 'new', 'nice', 'night', 'order', 'ordered', 'people', 'perfect', 'phoenix', 'pizza', 'place', 'pretty', 'prices', 'really', 'recommend', 'restaurant', 'right', 'said', 'salad', 'sandwich', 'sauce', 'say', 'service', 'staff', 'store', 'sure', 'table', 'thing', 'things', 'think', 'time', 'times', 'took', 'town', 'tried', 'try', 've', 'wait', 'want', 'way', 'went', 'wine', 'work', 'worth', 'years']\n"
     ]
    }
   ],
   "source": [
    "# All 100 features\n",
    "print((vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Just like with all other models, more features does not mean a better model. So, we must tune our feature generator to remove features whose predictive capability is none or very low.\n",
    "\n",
    "In this case, there is roughly a 1.6% increase in accuracy when we double the n-gram size and increase our max features by 1,000-fold. Note that if we restrict it to only unigrams, then the accuracy increases even more! So, bigrams were very likely adding more noise than signal. \n",
    "\n",
    "In the end, by only using 16,000 unigram features we came away with a much smaller, simpler, and easier-to-think-about model which also resulted in higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-grams and 2-grams, up to 100K features:\n",
      "('Features: ', 100000)\n",
      "('Accuracy: ', 0.8855185909980431)\n",
      "\n",
      "1-grams only, up to 100K features:\n",
      "('Features: ', 16825)\n",
      "('Accuracy: ', 0.9187866927592955)\n"
     ]
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams, and limit the number of features.\n",
    "\n",
    "print('1-grams and 2-grams, up to 100K features:')\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "tokenize_test(vect)\n",
    "\n",
    "print()\n",
    "print('1-grams only, up to 100K features:')\n",
    "vect = CountVectorizer(ngram_range=(1, 1), max_features=100000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `min_df`: Float in range [0.0, 1.0] or int, default=1\n",
    "    - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 43957)\n",
      "('Accuracy: ', 0.9324853228962818)\n"
     ]
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams, and only include terms that appear at least two times.\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stemming and Lemmatisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Introduction to TextBlob\n",
    "\n",
    "You should already have downloaded TextBlob, a Python library used to explore common NLP tasks. If you haven’t, please install using `pip`. We’ll be using this to organize our corpora for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "# Print the first review.\n",
    "print((yelp_best_worst.text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Save it as a TextBlob object.\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "review = TextBlob(yelp_best_worst.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the words.\n",
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"My wife took me here on my birthday for breakfast and it was excellent.\"),\n",
       " Sentence(\"The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.\"),\n",
       " Sentence(\"Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.\"),\n",
       " Sentence(\"It looked like the place fills up pretty quickly so the earlier you get here the better.\"),\n",
       " Sentence(\"Do yourself a favor and get their Bloody Mary.\"),\n",
       " Sentence(\"It was phenomenal and simply the best I've ever had.\"),\n",
       " Sentence(\"I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.\"),\n",
       " Sentence(\"It was amazing.\"),\n",
       " Sentence(\"While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.\"),\n",
       " Sentence(\"It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.\"),\n",
       " Sentence(\"It was the best \"toast\" I've ever had.\"),\n",
       " Sentence(\"Anyway, I can't wait to go back!\")]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the sentences.\n",
    "review.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "Stemming is a crude process of removing common endings from sentences, such as \"s\", \"es\", \"ly\", \"ing\", and \"ed\".\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form.\n",
    "\n",
    "\n",
    "- **Why:** This intelligently reduces the number of features by grouping together (hopefully) related words.\n",
    "\n",
    "\n",
    "- **Notes:**\n",
    "    - Stemming uses a simple and fast rule-based approach.\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing).\n",
    "    - Some search engines treat words with the same stem as synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excel', 'the', 'weather', 'was', 'perfect', 'which', 'made', 'sit', 'outsid', 'overlook', 'their', 'ground', 'an', 'absolut', 'pleasur', 'our', 'waitress', 'was', 'excel', 'and', 'our', 'food', 'arriv', 'quick', 'on', 'the', 'semi-busi', 'saturday', 'morn', 'it', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretti', 'quick', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'bloodi', 'mari', 'it', 'was', 'phenomen', 'and', 'simpli', 'the', 'best', 'i', 've', 'ever', 'had', 'i', \"'m\", 'pretti', 'sure', 'they', 'onli', 'use', 'ingredi', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'it', 'was', 'amaz', 'while', 'everyth', 'on', 'the', 'menu', 'look', 'excel', 'i', 'had', 'the', 'white', 'truffl', 'scrambl', 'egg', 'veget', 'skillet', 'and', 'it', 'was', 'tasti', 'and', 'delici', 'it', 'came', 'with', '2', 'piec', 'of', 'their', 'griddl', 'bread', 'with', 'was', 'amaz', 'and', 'it', 'absolut', 'made', 'the', 'meal', 'complet', 'it', 'was', 'the', 'best', 'toast', 'i', 've', 'ever', 'had', 'anyway', 'i', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# Initialize stemmer.\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Stem each word.\n",
    "print([stemmer.stem(word) for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some examples you can see are \"excellent\" stemmed to \"excel\" and \"amazing\" stemmed to \"amaz\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lemmatisation is a more refined process that uses specific language and grammar rules to derive the root of a word.  \n",
    "\n",
    "This is useful for words that do not share an obvious root such as \"better\" and \"best\".\n",
    "\n",
    "- **What:** Lemmatisation derives the canonical form (\"lemma\") of a word.\n",
    "- **Why:** It can be better than stemming.\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'wa', 'excellent', 'The', 'weather', 'wa', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'wa', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'wa', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredient', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'wa', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'look', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'egg', 'vegetable', 'skillet', 'and', 'it', 'wa', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'piece', 'of', 'their', 'griddled', 'bread', 'with', 'wa', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'wa', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# Assume every word is a noun.\n",
    "print([word.lemmatize() for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some examples you can see are \"filled\" lemmatized to \"fill\" and \"was\" lemmatized to \"wa\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'take', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'be', 'excellent', 'The', 'weather', 'be', 'perfect', 'which', 'make', 'sit', 'outside', 'overlook', 'their', 'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'be', 'excellent', 'and', 'our', 'food', 'arrive', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'be', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'have', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'be', 'amaze', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'look', 'excellent', 'I', 'have', 'the', 'white', 'truffle', 'scramble', 'egg', 'vegetable', 'skillet', 'and', 'it', 'be', 'tasty', 'and', 'delicious', 'It', 'come', 'with', '2', 'piece', 'of', 'their', 'griddle', 'bread', 'with', 'be', 'amaze', 'and', 'it', 'absolutely', 'make', 'the', 'meal', 'complete', 'It', 'be', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'have', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# Assume every word is a verb.\n",
    "print([word.lemmatize(pos='v') for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some examples you can see are \"was\" lemmatized to \"be\" and \"arrived\" lemmatized to \"arrive\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**More Lemmatization and Stemming Examples**\n",
    "\n",
    "|Lemmatization|Stemming|\n",
    "|-------------|---------|\n",
    "|shouted → shout|badly → bad|\n",
    "|best → good|computing → comput|\n",
    "|better → good|computed → comput|\n",
    "|good → good|wipes → wip|\n",
    "|wiping → wipe|wiped → wip|\n",
    "|hidden → hide|wiping → wip|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function that accepts text and returns a list of lemmas.\n",
    "def split_into_lemmas(text):\n",
    "    text = str(text, 'utf-8').lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With all the available options for `CountVectorizer()`, you may wonder how to decide which to use! It's true that you can sometimes reason about which preprocessing techniques might work best. However, you will often not know for sure without trying out many different combinations and comparing their accuracies. \n",
    "\n",
    "> Keep in mind that you should constantly be thinking about the result of each preprocessing step instead of blindly trying them without thinking. Does each type of preprocessing \"makes sense\" with the input data you are using? Is it likely to keep intact the signal and remove noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Term Frequency–Inverse Document Frequency (TF–IDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "While a Count Vectorizer simply totals up the number of times a \"word\" appears in a document, the more complex TF-IDF Vectorizer analyzes the uniqueness of words between documents to find distinguishing characteristics. \n",
    "     \n",
    "- **What:** Term frequency–inverse document frequency (TF–IDF) computes the \"relative frequency\" with which a word appears in a document, compared to its frequency across all documents.\n",
    "\n",
    "\n",
    "- **Why:** It's more useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents).\n",
    "\n",
    "\n",
    "- **Notes:** It's used for search-engine scoring, text summarization, and document clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term frequency\n",
    "vect = CountVectorizer()\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    1     3   2       1        1    1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document frequency\n",
    "vect = CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "pd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me  please  tonight  you\n",
       "0  0.0  0.333333  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.333333  0.5     0.0      0.0  0.0\n",
       "2  0.0  0.333333  0.5     2.0      0.0  0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term frequency–inverse document frequency (simple version)\n",
    "tf/df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The higher the TF–IDF value, the more \"important\" the word is to that specific document. \n",
    "\n",
    "Here, \"cab\" is the most important and unique word in document 1, while \"please\" is the most important and unique word in document 2. \n",
    "\n",
    "TF–IDF is often used for training as a replacement for word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Using TF–IDF to Summarize a Yelp Review\n",
    "\n",
    "Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF–IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28880)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a document-term matrix using TF–IDF.\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit transform Yelp data.\n",
    "dtm = vect.fit_transform(yelp.text)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    \n",
    "    # Choose a random review that is at least 300 characters.\n",
    "    review_length = 0\n",
    "    while review_length < 300:\n",
    "        review_id = np.random.randint(0, len(yelp))\n",
    "        review_text = yelp.text[review_id]\n",
    "        #review_text = unicode(yelp.text[review_id], 'utf-8')\n",
    "        review_length = len(review_text)\n",
    "    \n",
    "    # Create a dictionary of words and their TF–IDF scores.\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "    \n",
    "    # Print words with the top five TF–IDF scores.\n",
    "    print('TOP SCORING WORDS:')\n",
    "    top_scores = sorted(list(word_scores.items()), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print(word)\n",
    "    \n",
    "    # Print five random words.\n",
    "    print(('\\n' + 'RANDOM WORDS:'))\n",
    "    random_words = np.random.choice(list(word_scores.keys()), size=5, replace=False)\n",
    "    for word in random_words:\n",
    "        print(word)\n",
    "    \n",
    "    # Print the review.\n",
    "    print(('\\n' + review_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP SCORING WORDS:\n",
      "econolodge\n",
      "vaguely\n",
      "attracted\n",
      "overnight\n",
      "blocks\n",
      "\n",
      "RANDOM WORDS:\n",
      "hash\n",
      "restaurant\n",
      "try\n",
      "way\n",
      "toast\n",
      "\n",
      "Upon our overnight stay at Econolodge a few blocks down, the live lit sign along side of the street attracted me to try this restaurant.  I remember the menu vaguely, however did not forget the amazing breakfast special the other half and I ordered.  $6 for a plate of thick cut bacon, 2 eggs made your way, hash browns, and toast (white/wheat/rye, etc.)  Customer service was great, the hostess and waitress made us feel very comfortable!\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using TF-IDF in a Random Forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's use TfidfVectorizer as input to a random forest classifier, so that we can extract the most important features/words in 1-star and 5-star Yelp reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features = 1000, \n",
    "                             ngram_range=(1, 2), \n",
    "                             stop_words='english')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function that accepts text and returns the polarity.\n",
    "def detect_sentiment(text):\n",
    "    #return TextBlob(text.decode('utf-8')).sentiment.polarity\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Create a new DataFrame column for sentiment (Warning: SLOW!).\n",
    "yelp['sentiment'] = yelp.text.apply(detect_sentiment)\n",
    "\n",
    "\n",
    "# Create a DataFrame that only contains the 5-star and 1-star reviews.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define X and y\n",
    "feature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\n",
    "X = yelp_best_worst[feature_cols]\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# Calculate null accuracy.\n",
    "y_binary = np.where(y==5, 1, 0) # five stars become 1, one stars become 0\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3064, 1000)\n",
      "(1022, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Use TfIdfVectorizer with text column only.\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train.text)\n",
    "X_test_tfidf = vectorizer.transform(X_test.text)\n",
    "print((X_train_tfidf.shape))\n",
    "print((X_test_tfidf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators = 20)\n",
    "\n",
    "rf_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC [0.902669   0.89964496 0.91201693], Average AUC 0.9047769676372183\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(rf_model, X_train_tfidf, y_train, scoring='roc_auc')\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The models trains quite well. Let's look at the top ten most important features in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('great', 374), 0.039757671208807396)\n",
      "(('rude', 734), 0.025375012308550104)\n",
      "(('horrible', 424), 0.022752471480823877)\n",
      "(('worst', 984), 0.019435118945592716)\n",
      "(('bad', 54), 0.01891672646611991)\n",
      "(('told', 889), 0.01640042295059048)\n",
      "(('worse', 983), 0.01393440601080917)\n",
      "(('minutes', 557), 0.012424331414830169)\n",
      "(('awful', 50), 0.011876372880233466)\n",
      "(('wasn', 955), 0.009717947629589576)\n"
     ]
    }
   ],
   "source": [
    "def get_feature_importances(vocabulary, rf_importances, top_n):\n",
    "    vocab_features = sorted(vocabulary.items(), key=lambda x: x[1])\n",
    "    importances = zip(vocab_features, rf_importances)\n",
    "    \n",
    "    for z in sorted(importances, key=lambda x: abs(x[1]), reverse=True)[:top_n]:\n",
    "        print(z)\n",
    "\n",
    "get_feature_importances(vectorizer.vocabulary_, rf_model.feature_importances_, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Understanding how positive or negative a review is. There are many ways in practice to compute a sentiment value. For example:\n",
    "\n",
    "- Have a list of \"positive\" words and a list of \"negative\" words and count how many occur in a document. \n",
    "\n",
    "\n",
    "- Train a classifier given many examples of \"positive\" documents and \"negative\" documents. \n",
    "    - Note that this technique is often just an automated way to derive the first (e.g., using bag-of-words with logistic regression, a coefficient is assigned to each word!).\n",
    "\n",
    "For the most accurate sentiment analysis, you will want to train a custom sentiment model based on documents that are particular to your application. \n",
    "\n",
    "Generic models (such as the one we are about to use!) often do not work as well as hoped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40246913580246907"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polarity ranges from -1 (most negative) to 1 (most positive).\n",
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1141fccf8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Box plot of sentiment grouped by stars\n",
    "yelp.boxplot(column='sentiment', by='stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254    Our server Gary was awesome. Food was amazing....\n",
       "347    3 syllables for this place. \\nA-MAZ-ING!\\n\\nTh...\n",
       "420                                    LOVE the food!!!!\n",
       "459    Love it!!! Wish we still lived in Arizona as C...\n",
       "679                                     Excellent burger\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviews with most positive sentiment\n",
    "yelp[yelp.sentiment == 1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773     This was absolutely horrible. I got the suprem...\n",
       "1517                  Nasty workers and over priced trash\n",
       "3266    Absolutely awful... these guys have NO idea wh...\n",
       "4766                                       Very bad food!\n",
       "5812        I wouldn't send my worst enemy to this place.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviews with most negative sentiment\n",
    "yelp[yelp.sentiment == -1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Widen the column display.\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Negative sentiment in a 5-star review\n",
    "\n",
    "#yelp[(yelp.stars == 5) & (yelp.sentiment < -0.3)].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"assets/images/yelp_sentiment_1.png\" style=\"width:120%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Positive sentiment in a 1-star review\n",
    "\n",
    "#yelp[(yelp.stars == 1) & (yelp.sentiment > 0.5)].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"assets/images/yelp_sentiment_2.png\" style=\"width:120%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the column display width.\n",
    "pd.reset_option('max_colwidth')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
