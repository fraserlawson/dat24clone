{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What were some benefits of using linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- intuitive algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Could we use it for **classification**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Idea: for a binary problem, why not use linear regression to model the **probability** that a data point is in one class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- by definition we would also model the probability of it being in the other class too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What decision would we need to make for this to work for prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- a cutoff point i.e. a probability above which we say class 1 vs. 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Linear regression itself does not return probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But with a simple transformation we can 'squash' our line to be between 0 and 1:\n",
    "\n",
    "$$ \\text{p} = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Where $z$ is our linear model:\n",
    "\n",
    "$$ z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 ... $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, [sigmoid(x) for x in x])\n",
    "ax.hlines(0.5, -5, 5, colors=\"r\", linestyles=\"--\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So what is our linear equation modelling now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression**: Continuous response is modeled as a linear combination of the features.\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression**: Log odds of a categorical response being \"true\" is modeled as a linear combination of the features.\n",
    "\n",
    "$$\\log \\left({p\\over 1-p}\\right) = \\beta_0 + \\beta_1x$$\n",
    "\n",
    "This is called the logit function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is the \"log odds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When we say the \"odds\" of something, what do we mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The odds are the probability of something happening divided by the probability of it **not** happening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Odds\n",
    "\n",
    "$$ \\text{odds} = \\frac{p}{1-p} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ p = \\frac{\\text{odds}}{1+\\text{odds}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The equation can be rearranged into the logistic function.\n",
    "\n",
    "$$\\hat{p} = \\frac{e^{\\beta_0 + \\beta_1x}} {1 + e^{\\beta_0 + \\beta_1x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words:\n",
    "\n",
    "- Logistic regression outputs the probabilities of a specific class.\n",
    "- Those probabilities can be converted into class predictions.\n",
    "\n",
    "The logistic function has some nice properties:\n",
    "\n",
    "- Takes on an \"s\" shape\n",
    "- Output is bounded by 0 and 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's look at a simple example of logistic regression, again using our Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df_iris[\"target\"] = iris.target\n",
    "df_iris[\"target\"] = df_iris[\"target\"].map({idx:name for idx, name in enumerate(iris.target_names)})\n",
    "df_iris = df_iris[df_iris[\"target\"] != \"versicolor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "virginica    50\n",
       "setosa       50\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_iris[[\"petal length (cm)\"]]\n",
    "y = df_iris[\"target\"]\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Convert targets to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit_transform(y)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .ravel() \"flattens\" the output, because LabelBinarizer gives us arrays\n",
    "y = lb.fit_transform(y).ravel()\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.61229398] [[1.21564286]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lr = LogisticRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=424242,\n",
    "                                                    stratify=y)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(lr.intercept_, lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- coefficients are on the **log odds** scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- so an increase in 1 unit of petal length increases the log odds of being `versicolor` by 1.215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### While log odds values are linear..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.18100826] [0.0346346] [1.25027746] [2.46592032]\n"
     ]
    }
   ],
   "source": [
    "log_odds_petal_2_cm = lr.intercept_ + 2*lr.coef_[0][0]\n",
    "log_odds_petal_3_cm = lr.intercept_ + 3*lr.coef_[0][0]\n",
    "log_odds_petal_4_cm = lr.intercept_ + 4*lr.coef_[0][0]\n",
    "log_odds_petal_5_cm = lr.intercept_ + 5*lr.coef_[0][0]\n",
    "\n",
    "print(log_odds_petal_2_cm, log_odds_petal_3_cm, log_odds_petal_4_cm, log_odds_petal_5_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Probabilities are not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds_to_p(odds):\n",
    "    return odds / (1 + odds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23487096] [0.50865779] [0.77734789] [0.92171791]\n"
     ]
    }
   ],
   "source": [
    "print(odds_to_p(np.exp(log_odds_petal_2_cm)),\n",
    "      odds_to_p(np.exp(log_odds_petal_3_cm)),\n",
    "      odds_to_p(np.exp(log_odds_petal_4_cm)),\n",
    "      odds_to_p(np.exp(log_odds_petal_5_cm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What to remember"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- coefficients change the **log odds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- you can convert log odds to probabilities, but the odds-to-probability scale is **not linear**\n",
    "    - a 1 unit increase in log odds doesn't always give the same increase in probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- positive coefficients increase log odds and therefore increase probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- negative coefficients decrease log odds and therefore decrease probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have covered how this works for binary classification problems (two response classes). But what about multi-class classification problems (more than two response classes)?\n",
    "\n",
    "- The most common solution for classification models is \"one-vs-all\" (also known as \"one-vs-rest\"): Decompose the problem into multiple binary classification problems: this is the basis for support vector machines.\n",
    "\n",
    "\n",
    "- Multinomial logistic regression, on the other hand, can solve this as a single problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Multinomial logistic regression is known by a variety of other names, including polytomous LR, multiclass LR, softmax regression, multinomial logit, the maximum entropy (MaxEnt) classifier, and the conditional maximum entropy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look at the glass attribute data from the UCI machine learning website. The columns are different measurements of properties of glass that can be used to identify the glass type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.51761</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.51618</td>\n",
       "      <td>13.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RI     Na    Mg    Al     Si     K    Ca   Ba   Fe  Type\n",
       "0  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0     1\n",
       "1  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0     1\n",
       "2  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0     1\n",
       "3  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0     1\n",
       "4  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0     1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glass_data = pd.read_csv('assets/data/glass.csv')\n",
    "glass_data_headers = [\"RI\", \"Na\", \"Mg\", \"Al\", \"Si\", \"K\", \"Ca\", \"Ba\", \"Fe\", \"Type\"]\n",
    "glass_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will use this dataset to build a multinomial logistic regression model to predict the glass type.\n",
    "\n",
    "First, let's do a train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(glass_data[glass_data_headers[:-1]],\n",
    "                                                    glass_data[glass_data_headers[-1]], \n",
    "                                                    test_size=0.3)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, let's build a multinomial logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train multinomial logistic regression model\n",
    "mul_lr = LogisticRegression(multi_class='multinomial', solver='newton-cg').fit(train_x, train_y)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And how well does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic regression Train Accuracy ::  0.6375838926174496\n",
      "Multinomial Logistic regression Test Accuracy ::  0.5692307692307692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Multinomial Logistic regression Train Accuracy :: \", accuracy_score(train_y, mul_lr.predict(train_x)))\n",
    "print(\"Multinomial Logistic regression Test Accuracy :: \", accuracy_score(test_y, mul_lr.predict(test_x)))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"imbalanced\"> </a>\n",
    "\n",
    "# Imbalanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What can you do when there is a class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Balance the training set by either:\n",
    "    - Oversampling the minority class  \n",
    "    - Undersampling the majority class   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- tell your algorithm to weight the classes differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "LogisticRegression(class_weight=\"balanced\")\n",
    "LogisticRegression(class_weight={0: 100, 1: 200})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recommended reading: [https://www.svds.com/learning-imbalanced-classes](https://www.svds.com/learning-imbalanced-classes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"performance\"> </a>\n",
    "\n",
    "# Measuring Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Accuracy Paradox\n",
    "\n",
    "Accuracy is a very intuitive metric â€” it's a lot like an exam score where you get total correct/total attempted. However, accuracy is often a poor metric in application. There are many reasons for this:\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Imbalanced problems problems with 95% positives in the baseline will have 95% accuracy even with no predictive power.\n",
    "\n",
    "\n",
    "  - This is the paradox; pursuing accuracy often means predicting the most common class rather than doing the most useful work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Recap**: what are the two types of error we can make in an experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Type I**: false positive (doctor telling a man he's pregnant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Type II**: false negative (doctor telling a heavily pregnant lady that she's not pregnant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Assessing classification performance is a trade-off between these two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Sensitivity**: \"true positive rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\text{Sensitivity} = \\frac{\\sum{\\text{True Positives}}}{\\sum{\\text{Actual Positives}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Perfect sensitivity = \"all positive cases were found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Specificity**: \"true negative rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\text{Specificity} = \\frac{\\sum{\\text{True Negatives}}}{\\sum{\\text{Actual Negatives}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Perfect specificity: \"all negative cases were found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Border control\n",
    "\n",
    "How could we achieve \"perfect sensitivity\" (i.e. 100% true positive rate)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- \"everyone is a terrorist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 100% sensitivity (guaranteed to catch all terrorists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 0% specificity! Impossible to get any true negatives!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What about \"perfect specificity\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- \"no one is a terrorist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 100% specificity (all non-terrorists are let in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 0% sensitivity (no terrorists caught)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A good classifier trades off these two, and a good measure of classification performance measures this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Some of the most useful metrics for addressing these problems are:**\n",
    "    \n",
    "- **Classification accuracy/error**\n",
    "  - Classification accuracy is the percentage of correct predictions (higher is better).\n",
    "  - Classification error is the percentage of incorrect predictions (lower is better).\n",
    "  - Easiest classification metric to understand.\n",
    "  \n",
    "\n",
    "- **Confusion matrix**\n",
    "  - Gives you a better understanding of how your classifier is performing.\n",
    "  - Allows you to calculate sensitivity, specificity, and many other metrics that might match your business objective better than accuracy.\n",
    "  - Precision and recall are good for balancing misclassification costs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trading True Positives and True Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By default, and with respect to the underlying assumptions of logistic regression, we predict a positive class when the probability of the class is greater than .5 and predict a negative class otherwise.\n",
    "\n",
    "What if we decide to use .3 as a threshold for picking the positive class? Is that even allowed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This turns out to be a useful strategy. By setting a lower probability threshold we will predict more positive classes. Which means we will predict more true positives, but fewer true negatives.\n",
    "\n",
    "Making this trade-off is important in applications that have imbalanced penalties for misclassification.\n",
    "\n",
    "We can visually explore this trade off using ROC curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ROC Curve & AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The ROC curve is created by plotting the true positive rate against the false positive rate at various model threshold settings.\n",
    "\n",
    "Area Under the Curve (AUC) summarizes the impact of TPR and FPR in one single value.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - Good for ranking and prioritization problems.\n",
    "  \n",
    "  \n",
    "  - Allows you to visualize the performance of your classifier across all possible classification thresholds, thus helping you to choose a threshold that appropriately balances sensitivity and specificity.\n",
    "  \n",
    "  \n",
    "  - Still useful when there is high class imbalance (unlike classification accuracy/error).\n",
    "  \n",
    "  \n",
    "  - Harder to use when there are more than two response classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There can be a variety of points on an ROC curve.\n",
    "\n",
    "<img src=\"assets/images/roc_1.png\" style=\"width:60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can begin by plotting an individual TPR/FPR pair for one threshold.\n",
    "\n",
    "<img src=\"assets/images/roc_2.png\" style=\"width:60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can continue adding pairs for different thresholds\n",
    "\n",
    "<img src=\"assets/images/roc_3.png\" style=\"width:60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can continue adding pairs for different thresholds\n",
    "\n",
    "<img src=\"assets/images/roc_4.png\" style=\"width:60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we create a full curve that is described by TPR and FPR.\n",
    "<img src=\"assets/images/roc_5.png\" style=\"width:60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With this curve, we can find the Area Under the Curve (AUC).\n",
    "\n",
    "<img src=\"assets/images/roc_6.png\" style=\"width:60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Advanced Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Precision\n",
    "\n",
    "\"Of the things we predicted as positive, how many were **actually** positive?\"\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\sum{\\text{True Positives}}}{\\sum{\\text{All Predicted Positives}}}  = \\frac{\\sum{\\text{True Positives}}}{\\sum{\\text{True Positives}} + \\sum{\\text{False Positives}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Recall\n",
    "\n",
    "\"Of all possible positive cases, how many did we predict as positive?\"\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\sum{\\text{True Positives}}}{\\sum{\\text{All Positives}}}  = \\frac{\\sum{\\text{True Positives}}}{\\sum{\\text{True Positives}} + \\sum{\\text{False Negatives}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](assets/images/precision-recall.png)\n",
    "\n",
    "from: [https://www.quora.com/What-is-the-best-way-to-understand-the-terms-precision-and-recall](https://www.quora.com/What-is-the-best-way-to-understand-the-terms-precision-and-recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can even combine precision & recall into a single metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\text{F1 score} = 2\\frac{\\text{Precision}\\times\\text{Recall}}{\\text{Precision}+\\text{Recall}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Retrain logistic regression for two harder iris classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_iris_2 = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df_iris_2[\"target\"] = iris.target\n",
    "df_iris_2[\"target\"] = df_iris_2[\"target\"].map({idx:name for idx, name in enumerate(iris.target_names)})\n",
    "df_iris_2 = df_iris_2[df_iris_2[\"target\"] != \"setosa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "versicolor    50\n",
       "virginica     50\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iris_2[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lb2 = LabelBinarizer()\n",
    "\n",
    "X_2 = df_iris_2[[\"petal length (cm)\"]]\n",
    "y_2 = lb2.fit_transform(df_iris_2[\"target\"]).ravel()\n",
    "\n",
    "lr_2 = LogisticRegression()\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2,\n",
    "                                                            y_2,\n",
    "                                                            test_size=0.25,\n",
    "                                                            random_state=424242,\n",
    "                                                            stratify=y_2)\n",
    "lr_2.fit(X_train_2, y_train_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred_2 = lr_2.predict(X_test_2)\n",
    "\n",
    "acc = accuracy_score(y_test_2, y_pred_2)\n",
    "prec = precision_score(y_test_2, y_pred_2)\n",
    "rec = recall_score(y_test_2, y_pred_2)\n",
    "f1 = f1_score(y_test_2, y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  2]\n",
      " [ 0 13]]\n",
      "Accuracy: 0.92\tPrecision: 0.8666666666666667\tRecall: 1.0\tF-1: 0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test_2, y_pred_2))\n",
    "\n",
    "print(f\"Accuracy: {acc}\\tPrecision: {prec}\\tRecall: {rec}\\tF-1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](assets/images/roc_curve.png)\n",
    "\n",
    "from [https://www.ncss.com/software/ncss/roc-curves-ncss](https://www.ncss.com/software/ncss/roc-curves-ncss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9166666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc = roc_auc_score(y_test_2, y_pred_2)\n",
    "print(f\"AUC: {auc}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
